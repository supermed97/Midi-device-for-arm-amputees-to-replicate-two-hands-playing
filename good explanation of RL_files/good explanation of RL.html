<!DOCTYPE html>
<!-- saved from url=(0132)https://medium.com/@m.alzantot/deep-reinforcement-learning-demysitifed-episode-2-policy-iteration-value-iteration-and-q-978f9e89ddaa -->
<html lang="en" data-rh="lang"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><script async="" src="./good explanation of RL_files/branch-latest.min.js.download"></script><script async="" src="./good explanation of RL_files/analytics.js.download"></script><script>!function(c,f){var t,o,i,e=[],r={passive:!0,capture:!0},n=new Date,a="pointerup",u="pointercancel";function p(n,e){t||(t=e,o=n,i=new Date,w(f),s())}function s(){0<=o&&o<i-n&&(e.forEach(function(n){n(o,t)}),e=[])}function l(n){if(n.cancelable){var e=(1e12<n.timeStamp?new Date:performance.now())-n.timeStamp;"pointerdown"==n.type?function(n,e){function t(){p(n,e),i()}function o(){i()}function i(){f(a,t,r),f(u,o,r)}c(a,t,r),c(u,o,r)}(e,n):p(e,n)}}function w(e){["click","mousedown","keydown","touchstart","pointerdown"].forEach(function(n){e(n,l,r)})}w(c),self.perfMetrics=self.perfMetrics||{},self.perfMetrics.onFirstInputDelay=function(n){e.push(n),s()}}(addEventListener,removeEventListener)</script><title>Deep Reinforcement Learning Demysitifed (Episode 2) ‚Äî Policy Iteration, Value Iteration and Q-learning</title><meta data-rh="true" name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1"><meta data-rh="true" name="theme-color" content="#000000"><meta data-rh="true" name="twitter:app:name:iphone" content="Medium"><meta data-rh="true" name="twitter:app:id:iphone" content="828256236"><meta data-rh="true" property="al:ios:app_name" content="Medium"><meta data-rh="true" property="al:ios:app_store_id" content="828256236"><meta data-rh="true" property="al:android:package" content="com.medium.reader"><meta data-rh="true" property="fb:app_id" content="542599432471018"><meta data-rh="true" property="og:site_name" content="Medium"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2018-10-08T01:41:28.489Z"><meta data-rh="true" name="title" content="Deep Reinforcement Learning Demysitifed (Episode 2) ‚Äî Policy Iteration, Value Iteration and Q-learning"><meta data-rh="true" property="og:title" content="Deep Reinforcement Learning Demysitifed (Episode 2) ‚Äî Policy Iteration, Value Iteration and‚Ä¶"><meta data-rh="true" property="twitter:title" content="Deep Reinforcement Learning Demysitifed (Episode 2) ‚Äî Policy Iteration, Value Iteration and‚Ä¶"><meta data-rh="true" name="twitter:site" content="@Medium"><meta data-rh="true" name="twitter:app:url:iphone" content="medium://p/978f9e89ddaa"><meta data-rh="true" property="al:android:url" content="medium://p/978f9e89ddaa"><meta data-rh="true" property="al:ios:url" content="medium://p/978f9e89ddaa"><meta data-rh="true" property="al:android:app_name" content="Medium"><meta data-rh="true" name="description" content="In previous two articles, we introduced reinforcement learning definition, examples, and simple solving strategies using random policy search and genetic algorithms. In practice, random search does‚Ä¶"><meta data-rh="true" property="og:description" content="In previous two articles, we introduced reinforcement learning definition, examples, and simple solving strategies using random policy‚Ä¶"><meta data-rh="true" property="twitter:description" content="In previous two articles, we introduced reinforcement learning definition, examples, and simple solving strategies using random policy‚Ä¶"><meta data-rh="true" property="og:url" content="https://medium.com/@m.alzantot/deep-reinforcement-learning-demysitifed-episode-2-policy-iteration-value-iteration-and-q-978f9e89ddaa"><meta data-rh="true" property="al:web:url" content="https://medium.com/@m.alzantot/deep-reinforcement-learning-demysitifed-episode-2-policy-iteration-value-iteration-and-q-978f9e89ddaa"><meta data-rh="true" property="og:image" content="https://miro.medium.com/freeze/max/600/1*riQcuYHUPeg9adyv24kPHg.gif"><meta data-rh="true" name="twitter:image:src" content="https://miro.medium.com/freeze/max/600/1*riQcuYHUPeg9adyv24kPHg.gif"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="article:author" content="https://medium.com/@m.alzantot"><meta data-rh="true" name="twitter:creator" content="@m_alzantot"><meta data-rh="true" name="author" content="Moustafa Alzantot"><meta data-rh="true" name="robots" content="index,follow"><meta data-rh="true" name="referrer" content="unsafe-url"><meta data-rh="true" name="twitter:label1" value="Reading time"><meta data-rh="true" name="twitter:data1" value="11 min read"><meta data-rh="true" name="parsely-post-id" content="978f9e89ddaa"><link data-rh="true" rel="search" type="application/opensearchdescription+xml" title="Medium" href="https://medium.com/osd.xml"><link data-rh="true" rel="apple-touch-icon" sizes="152x152" href="https://cdn-images-1.medium.com/fit/c/152/152/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="apple-touch-icon" sizes="120x120" href="https://cdn-images-1.medium.com/fit/c/120/120/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="apple-touch-icon" sizes="76x76" href="https://cdn-images-1.medium.com/fit/c/76/76/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="apple-touch-icon" sizes="60x60" href="https://cdn-images-1.medium.com/fit/c/60/60/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="mask-icon" href="https://cdn-static-1.medium.com/_/fp/icons/monogram-mask.KPLCSFEZviQN0jQ7veN2RQ.svg" color="#171717"><link data-rh="true" id="glyph_link" rel="stylesheet" type="text/css" href="./good explanation of RL_files/m2.css"><link data-rh="true" rel="author" href="https://medium.com/@m.alzantot"><link data-rh="true" rel="canonical" href="https://medium.com/@m.alzantot/deep-reinforcement-learning-demysitifed-episode-2-policy-iteration-value-iteration-and-q-978f9e89ddaa"><link data-rh="true" rel="alternate" href="android-app://com.medium.reader/https/medium.com/p/978f9e89ddaa"><link data-rh="true" rel="icon" href="https://cdn-static-1.medium.com/_/fp/icons/favicon-rebrand-medium.3Y6xpZ-0FSdWDnPM3hSBIA.ico"><script data-rh="true">(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-24232453-2', 'auto');
ga('send', 'pageview');</script><link rel="preload" href="./good explanation of RL_files/16180790160.js.download" as="script"><style type="text/css" data-fela-rehydration="437" data-fela-type="STATIC">html{box-sizing:border-box}*, *:before, *:after{box-sizing:inherit}body{margin:0;padding:0;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;color:rgba(0,0,0,0.8);position:relative;min-height:100vh}h1, h2, h3, h4, h5, h6, dl, dd, ol, ul, menu, figure, blockquote, p, pre, form{margin:0}menu, ol, ul{padding:0;list-style:none;list-style-image:none}main{display:block}a{color:inherit;text-decoration:none}a, button, input{-webkit-tap-highlight-color:transparent}img, svg{vertical-align:middle}button{background:transparent;overflow:visible}button, input, optgroup, select, textarea{margin:0}</style><style type="text/css" data-fela-rehydration="437" data-fela-type="KEYFRAME">@-webkit-keyframes k1{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@-moz-keyframes k1{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@keyframes k1{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@-webkit-keyframes k2{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}@-moz-keyframes k2{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}@keyframes k2{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}@-webkit-keyframes k3{0%{transform:matrix(0.97, 0, 0, 1, 0, 12);opacity:0}20%{transform:matrix(0.99, 0, 0, 1, 0, 2);opacity:0.7}40%{transform:matrix(1, 0, 0, 1, 0, -1);opacity:1}70%{transform:matrix(1, 0, 0, 1, 0, 0);opacity:1}100%{transform:matrix(1, 0, 0, 1, 0, 0);opacity:1}}@-moz-keyframes k3{0%{transform:matrix(0.97, 0, 0, 1, 0, 12);opacity:0}20%{transform:matrix(0.99, 0, 0, 1, 0, 2);opacity:0.7}40%{transform:matrix(1, 0, 0, 1, 0, -1);opacity:1}70%{transform:matrix(1, 0, 0, 1, 0, 0);opacity:1}100%{transform:matrix(1, 0, 0, 1, 0, 0);opacity:1}}@keyframes k3{0%{transform:matrix(0.97, 0, 0, 1, 0, 12);opacity:0}20%{transform:matrix(0.99, 0, 0, 1, 0, 2);opacity:0.7}40%{transform:matrix(1, 0, 0, 1, 0, -1);opacity:1}70%{transform:matrix(1, 0, 0, 1, 0, 0);opacity:1}100%{transform:matrix(1, 0, 0, 1, 0, 0);opacity:1}}</style><style type="text/css" data-fela-rehydration="437" data-fela-type="RULE">.a{font-family:medium-content-sans-serif-font, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Open Sans", "Helvetica Neue", sans-serif}.b{font-weight:400}.c{background-color:rgba(255, 255, 255, 1)}.l{height:100vh}.m{width:100vw}.n{display:flex}.o{align-items:center}.p{justify-content:center}.q{fill:rgba(0, 0, 0, 0.84)}.r{display:block}.s{position:absolute}.t{top:0}.u{left:0}.v{right:0}.w{z-index:500}.x{box-shadow:0 4px 12px 0 rgba(0, 0, 0, 0.05)}.ag{max-width:1192px}.ah{min-width:0}.ai{width:100%}.aj{height:65px}.am{flex:1 0 auto}.an{font-family:'Helvetica Neue', sans-serif}.ao{font-size:20px}.ap{color:rgba(0, 0, 0, 0.84)}.aq{margin-top:7px}.ar{margin-left:10px}.as{display:-webkit-box}.at{-webkit-line-clamp:1}.au{-webkit-box-orient:vertical}.av{overflow:hidden}.ba{display:none}.bc{visibility:hidden}.bd{margin-left:16px}.be{color:rgba(2, 158, 116, 1)}.bf{fill:rgba(3, 168, 124, 1)}.bg{font-size:inherit}.bh{border:inherit}.bi{font-family:inherit}.bj{letter-spacing:inherit}.bk{font-weight:inherit}.bl{padding:0}.bm{margin:0}.bn:hover{cursor:pointer}.bo:hover{color:rgba(1, 143, 105, 1)}.bp:hover{fill:rgba(2, 158, 116, 1)}.bq:focus{outline:none}.br:disabled{cursor:default}.bs:disabled{color:rgba(140, 93, 255, 0.5)}.bt:disabled{fill:rgba(140, 93, 255, 0.5)}.bu{margin-left:15px}.bv{margin-top:1px}.bw{flex:0 0 auto}.bx{font-family:medium-content-sans-serif-font, "Lucida Grande", "Lucida Sans Unicode", "Lucida Sans", Geneva, Arial, sans-serif}.by{font-style:normal}.bz{line-height:20px}.ca{font-size:15.8px}.cb{letter-spacing:0px}.cc{color:rgba(0, 0, 0, 0.54)}.cd{fill:rgba(0, 0, 0, 0.54)}.ce{justify-content:flex-end}.cf{margin-top:16px}.cg{margin-bottom:16px}.ch{display:inherit}.ci{max-width:210px}.cj{text-overflow:ellipsis}.ck{white-space:nowrap}.cl{display:inline-block}.cm{border:none}.cn{outline:none}.co{font:inherit}.cp{font-size:16px}.cq{opacity:0}.cr{position:relative}.cs{width:0px}.ct{transition:width 140ms ease-in}.cu{color:inherit}.cv{fill:inherit}.cw:hover{color:rgba(0, 0, 0, 0.9)}.cx:hover{fill:rgba(0, 0, 0, 0.9)}.cy:disabled{color:rgba(0, 0, 0, 0.54)}.cz:disabled{fill:rgba(0, 0, 0, 0.54)}.da{margin-right:10px}.de{margin-right:16px}.df{margin:15px 0}.dg{padding:4px 12px}.dh{background:0}.di{border-color:rgba(0, 0, 0, 0.54)}.dj:hover{color:rgba(0, 0, 0, 0.97)}.dk:hover{fill:rgba(0, 0, 0, 0.97)}.dl:hover{border-color:rgba(0, 0, 0, 0.84)}.dm:disabled{fill:rgba(0, 0, 0, 0.76)}.dn:disabled{border-color:rgba(0, 0, 0, 0.2)}.do:disabled{cursor:inherit}.dp:disabled:hover{color:rgba(0, 0, 0, 0.54)}.dq:disabled:hover{fill:rgba(0, 0, 0, 0.76)}.dr:disabled:hover{border-color:rgba(0, 0, 0, 0.2)}.ds{border-radius:4px}.dt{border-width:1px}.du{border-style:solid}.dv{box-sizing:border-box}.dw{text-decoration:none}.dx{padding-bottom:10px}.dy{padding-top:10px}.dz{border-radius:50%}.ea{height:32px}.eb{width:32px}.ec{margin-bottom:0px}.ee{padding-left:24px}.ef{padding-right:24px}.eg{margin-left:auto}.eh{margin-right:auto}.ei{max-width:728px}.ej{flex-direction:column}.ek{top:calc(100vh + 100px)}.el{bottom:calc(100vh + 100px)}.em{width:10px}.en{pointer-events:none}.eo{word-break:break-word}.ep{word-wrap:break-word}.eq:after{display:block}.er:after{content:""}.es:after{clear:both}.et{max-width:680px}.eu{line-height:1.23}.ev{letter-spacing:0}.ew{font-family:medium-content-title-font, Georgia, Cambria, "Times New Roman", Times, serif}.fh{margin-bottom:-0.27em}.fn{margin-top:32px}.fo{justify-content:space-between}.fs{height:48px}.ft{width:48px}.fu{margin-left:12px}.fv{margin-bottom:2px}.fx{font-weight:300}.fy{max-height:20px}.fz:hover{text-decoration:underline}.ga{margin-left:8px}.gb{padding:0px 8px}.gc{border-color:rgba(3, 168, 124, 1)}.gd:hover{border-color:rgba(2, 158, 116, 1)}.ge{line-height:18px}.gf{font-size:15px}.gg{align-items:flex-end}.go{padding-right:6px}.gp{margin-right:8px}.gq{fill:rgba(0, 0, 0, 0.76)}.gr{margin-right:-6px}.gs{line-height:1.58}.gt{letter-spacing:-0.004em}.gu{font-family:medium-content-serif-font, Georgia, Cambria, "Times New Roman", Times, serif}.hf{margin-bottom:-0.46em}.hg{background-repeat:repeat-x}.hh{background-image:linear-gradient(to right,rgba(0, 0, 0, 0.84) 100%,rgba(0, 0, 0, 0.84) 0);background-image:url('data:image/svg+xml;utf8,<svg preserveAspectRatio="none" viewBox="0 0 1 1" xmlns="http://www.w3.org/2000/svg"><line x1="0" y1="0" x2="1" y2="1" stroke="rgba(0, 0, 0, 0.84)" /></svg>')}.hi{background-size:1px 1px}.hj{background-position:0 1.05em;background-position:0 calc(1em + 1px)}.hk{line-height:1.18}.hl{letter-spacing:-0.022em}.hm{font-weight:600}.hx{margin-bottom:-0.31em}.id{background-color:rgba(0, 0, 0, 0.05)}.ie{padding:2px 4px}.if{font-size:75%}.ig> strong{font-family:inherit}.ih{font-family:Menlo, Monaco, "Courier New", Courier, monospace}.ii{font-weight:700}.ij{font-style:italic}.ik{list-style-type:disc}.il{margin-left:30px}.im{padding-left:0px}.is{max-width:197px}.iy{clear:both}.iz{transition:opacity 100ms 400ms}.ja{height:100%}.jb{will-change:transform}.jc{transform:translateZ(0)}.jd{margin:auto}.je{padding-bottom:18.78172588832487%}.jf{filter:blur(20px)}.jg{transform:scale(1.1)}.jh{visibility:visible}.ji{background:rgba(255, 255, 255, 1)}.jj{max-width:1168px}.jk{transition:transform 300ms cubic-bezier(0.2, 0, 0.2, 1)}.jl{cursor:zoom-in}.jm{z-index:auto}.jn{padding-bottom:18.150684931506852%}.jo{max-width:948px}.jp{padding-bottom:22.362869198312236%}.jq{max-width:876px}.jr{padding-bottom:11.415525114155251%}.js{max-width:898px}.jt{padding-bottom:11.135857461024498%}.ju{max-width:456px}.jv{padding-bottom:14.473684210526315%}.jw{max-width:938px}.jx{padding-bottom:10.660980810234541%}.jy{max-width:1052px}.jz{padding-bottom:9.505703422053232%}.ka{max-width:1460px}.kb{padding-bottom:46.16438356164384%}.kc{line-height:1.12}.kl{margin-bottom:-0.28em}.km{max-width:856px}.kn{padding-bottom:39.018691588785046%}.ko{line-height:1.4}.kp{margin-top:10px}.kq{text-align:center}.kt{padding-bottom:NaN%}.ku{max-width:974px}.kv{padding-bottom:41.27310061601643%}.kw{max-width:1364px}.kx{padding-bottom:15.395894428152493%}.ky{max-width:600px}.kz{padding-bottom:66.66666666666667%}.la{will-change:opacity}.lb{position:fixed}.lc{width:188px}.ld{left:50%}.le{transform:translateX(406px)}.lf{top:calc(65px + 54px + 14px)}.li{top:calc(65px + 54px + 40px)}.lk{width:131px}.ll{padding-top:28px}.lm{margin-bottom:19px}.ln{margin-left:-3px}.lo{margin-right:5px}.lp{outline:0}.lq{border:0}.lr{user-select:none}.ls{cursor:pointer}.lt> svg{pointer-events:none}.lu:active{border-style:none}.lv{-webkit-user-select:none}.lw:focus{fill:rgba(0, 0, 0, 0.54)}.lx:hover{fill:rgba(0, 0, 0, 0.54)}.ly{margin-top:5px}.lz button{text-align:left}.ma{margin-top:40px}.mb{flex-wrap:wrap}.mc{margin-top:25px}.md{list-style-type:none}.me{margin-bottom:8px}.mf{border-radius:3px}.mg{padding:5px 10px}.mh{background:rgba(0, 0, 0, 0.05)}.mi{line-height:22px}.mj{margin-top:15px}.mk{border:1px solid rgba(0, 0, 0, 0.1)}.ml{height:60px}.mm{width:60px}.mz:hover{border-color:rgba(0, 0, 0, 0.54)}.na:active{border-style:solid}.nb{z-index:2}.nd{padding-right:8px}.ne{padding-top:32px}.nf{border-top:1px solid rgba(0, 0, 0, 0.1)}.ng{margin-bottom:25px}.nh{margin-bottom:32px}.ni{min-height:80px}.nn{height:80px}.no{width:80px}.np{padding-left:102px}.nr{text-transform:uppercase}.ns{letter-spacing:0.05em}.nt{margin-bottom:6px}.nu{font-size:28px}.nv{line-height:36px}.nw{max-width:555px}.nx{max-width:450px}.ny{font-size:18px}.nz{line-height:24px}.ob{padding-top:25px}.oc{color:rgba(0, 0, 0, 0.76)}.od{opacity:1}.oe{padding:20px}.of{border:1px solid rgba(3, 168, 124, 1)}.og{margin-top:64px}.oh{background-color:rgba(0, 0, 0, 0.02)}.oi{padding:60px 0}.oj{background-color:rgba(0, 0, 0, 0.9)}.pa{padding-bottom:48px}.pb{border-bottom:1px solid rgba(255, 255, 255, 0.54)}.pc{margin:0 -12px}.pd{margin:0 12px}.pe{flex:1 1 0}.pf{padding-bottom:12px}.pg:hover{color:rgba(255, 255, 255, 0.99)}.ph:hover{fill:rgba(255, 255, 255, 0.99)}.pi:disabled{color:rgba(255, 255, 255, 0.7)}.pj:disabled{fill:rgba(255, 255, 255, 0.7)}.pk{color:rgba(255, 255, 255, 0.98)}.pl{fill:rgba(255, 255, 255, 0.98)}.pm{text-align:inherit}.pn{font-size:21.6px}.po{letter-spacing:-0.32px}.pp{color:rgba(255, 255, 255, 0.7)}.pq{fill:rgba(255, 255, 255, 0.7)}.pr{text-decoration:underline}.ps{padding-bottom:8px}.pt{padding-top:8px}.pu{width:200px}.pw:disabled{color:rgba(3, 168, 124, 0.5)}.px:disabled{fill:rgba(3, 168, 124, 0.5)}.py{-webkit-user-select:none}</style><style type="text/css" data-fela-rehydration="437" data-fela-type="RULE" media="all and (min-width: 1080px)">.d{display:none}.af{margin:0 64px}.ff{font-size:40px}.fg{margin-top:0.78em}.fm{line-height:48px}.gn{margin-left:30px}.hd{font-size:21px}.he{margin-top:2em}.hv{font-size:26px}.hw{margin-top:1.72em}.ic{margin-top:0.86em}.ir{margin-top:1.05em}.ix{margin-top:56px}.kj{font-size:34px}.kk{margin-top:1.95em}.ox{padding-left:64px}.oy{padding-right:64px}.oz{max-width:1320px}</style><style type="text/css" data-fela-rehydration="437" data-fela-type="RULE" media="all and (max-width: 1079.98px)">.e{display:none}.gm{margin-left:30px}.kr{margin-left:auto}.ks{text-align:center}.ou{padding-left:64px}.ov{padding-right:64px}.ow{max-width:1080px}</style><style type="text/css" data-fela-rehydration="437" data-fela-type="RULE" media="all and (max-width: 903.98px)">.f{display:none}.dd{display:flex}.gl{margin-left:30px}.or{padding-left:48px}.os{padding-right:48px}.ot{max-width:904px}</style><style type="text/css" data-fela-rehydration="437" data-fela-type="RULE" media="all and (max-width: 727.98px)">.g{display:none}.ak{height:56px}.al{display:flex}.aw{font-size:14px}.ax{margin-top:0px}.ay{margin-left:4px}.az{-webkit-line-clamp:2}.bb{display:block}.db{margin-left:10px}.dc{margin-right:10px}.ed{margin-bottom:0px}.fq{margin-top:32px}.fr{flex-direction:column-reverse}.gj{margin-bottom:30px}.gk{margin-left:0px}.nj{margin-bottom:24px}.nk{align-items:center}.nl{width:102px}.nm{position:relative}.nq{padding-left:0}.oa{margin-top:24px}.ok{padding:32px 0}.oo{padding-left:24px}.op{padding-right:24px}.oq{max-width:728px}.pv{width:140px}</style><style type="text/css" data-fela-rehydration="437" data-fela-type="RULE" media="all and (max-width: 551.98px)">.h{display:none}.z{margin:0 24px}.ex{font-size:30px}.ey{margin-top:0.72em}.fi{line-height:40px}.fp{margin-top:32px}.fw{margin-bottom:0px}.gh{margin-bottom:30px}.gi{margin-left:0px}.gv{font-size:18px}.gw{margin-top:1.56em}.hn{font-size:24px}.ho{margin-top:1.23em}.hy{margin-top:0.67em}.in{margin-top:1.34em}.it{margin-top:40px}.kd{margin-top:1.2em}.ol{padding-left:24px}.om{padding-right:24px}.on{max-width:552px}</style><style type="text/css" data-fela-rehydration="437" data-fela-type="RULE" media="all and (min-width: 904px) and (max-width: 1079.98px)">.i{display:none}.ae{margin:0 64px}.fd{font-size:40px}.fe{margin-top:0.78em}.fl{line-height:48px}.hb{font-size:21px}.hc{margin-top:2em}.ht{font-size:26px}.hu{margin-top:1.72em}.ib{margin-top:0.86em}.iq{margin-top:1.05em}.iw{margin-top:56px}.kh{font-size:34px}.ki{margin-top:1.95em}</style><style type="text/css" data-fela-rehydration="437" data-fela-type="RULE" media="all and (min-width: 728px) and (max-width: 903.98px)">.j{display:none}.ac{margin:0 48px}.fb{font-size:40px}.fc{margin-top:0.78em}.fk{line-height:48px}.gz{font-size:21px}.ha{margin-top:2em}.hr{font-size:26px}.hs{margin-top:1.72em}.ia{margin-top:0.86em}.ip{margin-top:1.05em}.iv{margin-top:56px}.kf{font-size:34px}.kg{margin-top:1.95em}</style><style type="text/css" data-fela-rehydration="437" data-fela-type="RULE" media="all and (min-width: 552px) and (max-width: 727.98px)">.k{display:none}.ab{margin:0 24px}.ez{font-size:30px}.fa{margin-top:0.72em}.fj{line-height:40px}.gx{font-size:18px}.gy{margin-top:1.56em}.hp{font-size:24px}.hq{margin-top:1.23em}.hz{margin-top:0.67em}.io{margin-top:1.34em}.iu{margin-top:40px}.ke{margin-top:1.2em}</style><style type="text/css" data-fela-rehydration="437" data-fela-type="RULE" media="print">.y{display:none}</style><style type="text/css" data-fela-rehydration="437" data-fela-type="RULE" media="(prefers-reduced-motion: no-preference)">.lg{transition:opacity 200ms}.mn{transition:border-color 150ms ease}.mo::before{background:
      radial-gradient(circle, rgba(0, 0, 0, 0.84) 60%, transparent 70%)
    }.mp::before{border-radius:50%}.mq::before{content:""}.mr::before{display:block}.ms::before{z-index:0}.mt::before{left:0}.mu::before{height:100%}.mv::before{position:absolute}.mw::before{top:0}.mx::before{width:100%}.my:hover::before{animation:k2 2000ms infinite cubic-bezier(.1,.12,.25,1)}.nc{transition:fill 200ms ease}</style><style type="text/css" data-fela-rehydration="437" data-fela-type="RULE" media="all and (max-width: 1230px)">.lh{display:none}</style><style type="text/css" data-fela-rehydration="437" data-fela-type="RULE" media="all and (max-width: 1198px)">.lj{display:none}</style><script charset="utf-8" src="./good explanation of RL_files/vendors_tracing.7b3bfa92.chunk.js.download"></script><script charset="utf-8" src="./good explanation of RL_files/tracing.0d9b35c9.chunk.js.download"></script><script type="application/ld+json" data-rh="true">{"@context":"http:\u002F\u002Fschema.org","@type":"NewsArticle","image":["https:\u002F\u002Fmiro.medium.com\u002Ffreeze\u002Fmax\u002F1200\u002F1*riQcuYHUPeg9adyv24kPHg.gif"],"url":"https:\u002F\u002Fmedium.com\u002F@m.alzantot\u002Fdeep-reinforcement-learning-demysitifed-episode-2-policy-iteration-value-iteration-and-q-978f9e89ddaa","dateCreated":"2017-07-09T04:25:45.619Z","datePublished":"2017-07-09T04:25:45.619Z","dateModified":"2018-10-08T01:41:28.489Z","headline":"Deep Reinforcement Learning Demysitifed (Episode 2) ‚Äî Policy Iteration, Value Iteration and‚Ä¶","name":"Deep Reinforcement Learning Demysitifed (Episode 2) ‚Äî Policy Iteration, Value Iteration and‚Ä¶","description":"In previous two articles, we introduced reinforcement learning definition, examples, and simple solving strategies using random policy search and genetic algorithms. In practice, random search does‚Ä¶","identifier":"978f9e89ddaa","keywords":["Lite:true","Tag:Machine Learning","Tag:Reinforcement Learning","Tag:Deep Learning","Tag:Artificial Intelligence","Tag:OpenAI","Elevated:false","LockedPostSource:LOCKED_POST_SOURCE_NONE","LayerCake:0"],"author":{"@type":"Person","name":"Moustafa Alzantot","url":"https:\u002F\u002Fmedium.com\u002F@m.alzantot"},"creator":["Moustafa Alzantot"],"publisher":{"@type":"Organization","name":"Medium","url":"https:\u002F\u002Fmedium.com\u002F","logo":{"@type":"ImageObject","width":308,"height":60,"url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F308\u002F1*OMF3fSqH8t4xBJ9-6oZDZw.png"}},"mainEntityOfPage":"https:\u002F\u002Fmedium.com\u002F@m.alzantot\u002Fdeep-reinforcement-learning-demysitifed-episode-2-policy-iteration-value-iteration-and-q-978f9e89ddaa"}</script><script type="text/javascript" data-rh="true">(function(b,r,a,n,c,h,_,s,d,k){if(!b[n]||!b[n]._q){for(;s<_.length;)c(h,_[s++]);d=r.createElement(a);d.async=1;d.src="https://cdn.branch.io/branch-latest.min.js";k=r.getElementsByTagName(a)[0];k.parentNode.insertBefore(d,k);b[n]=h}})(window,document,"script","branch",function(b,r){b[r]=function(){b._q.push([r,arguments])}},{_q:[],_v:1},"addListener applyCode autoAppIndex banner closeBanner closeJourney creditHistory credits data deepview deepviewCta first getCode init link logout redeem referrals removeListener sendSMS setBranchViewData setIdentity track validateCode trackCommerceEvent logEvent".split(" "), 0);
branch.init('key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm', {metadata: {}, 'no_journeys': true, 'disable_exit_animation': true, 'disable_entry_animation': true, 'tracking_disabled': null}, function(err, data) {});</script></head><body><div id="root"><div class="a b c"><div class="d e f g h i j k"></div><script>document.domain = document.domain;</script><script>window.PARSELY = window.PARSELY || {autotrack: false}</script><nav class="r s t u v c w x y"><div><div class="n p"><div class="z ab ac ae af ag ah ai"><div class="aj n o ak al"><div class="r am w"><div class="n o"><a href="https://medium.com/?source=post_page-----978f9e89ddaa----------------------" aria-label="Homepage" rel="noopener"><div class="ba g"><svg height="22" width="112" viewBox="0 0 111.5 22" class="q"><path d="M56.3 19.5c0 .4 0 .5.3.7l1.5 1.4v.1h-6.5V19c-.7 1.8-2.4 3-4.3 3-3.3 0-5.8-2.6-5.8-7.5 0-4.5 2.6-7.6 6.3-7.6 1.6-.1 3.1.8 3.8 2.4V3.2c0-.3-.1-.6-.3-.7l-1.4-1.4V1l6.5-.8v19.3zm-4.8-.8V9.5c-.5-.6-1.2-.9-1.9-.9-1.6 0-3.1 1.4-3.1 5.7 0 4 1.3 5.4 3 5.4.8.1 1.6-.3 2-1zm9.1 3.1V9.4c0-.3-.1-.6-.3-.7l-1.4-1.5v-.1h6.5v12.5c0 .4 0 .5.3.7l1.4 1.4v.1h-6.5zm-.2-19.2C60.4 1.2 61.5 0 63 0c1.4 0 2.6 1.2 2.6 2.6S64.4 5.3 63 5.3a2.6 2.6 0 0 1-2.6-2.7zm22.5 16.9c0 .4 0 .5.3.7l1.5 1.4v.1h-6.5v-3.2c-.6 2-2.4 3.4-4.5 3.4-2.9 0-4.4-2.1-4.4-6.2 0-1.9 0-4.1.1-6.5 0-.3-.1-.5-.3-.7L67.7 7v.1H74v8c0 2.6.4 4.4 2 4.4.9-.1 1.7-.6 2.1-1.3V9.5c0-.3-.1-.6-.3-.7l-1.4-1.5v-.2h6.5v12.4zm22 2.3c0-.5.1-6.5.1-7.9 0-2.6-.4-4.5-2.2-4.5-.9 0-1.8.5-2.3 1.3.2.8.3 1.7.3 2.5 0 1.8-.1 4.2-.1 6.5 0 .3.1.5.3.7l1.5 1.4v.1H96c0-.4.1-6.5.1-7.9 0-2.7-.4-4.5-2.2-4.5-.9 0-1.7.5-2.2 1.3v9c0 .4 0 .5.3.7l1.4 1.4v.1h-6.5V9.5c0-.3-.1-.6-.3-.7l-1.4-1.5v-.2h6.5v3.1a4.6 4.6 0 0 1 4.6-3.4c2.2 0 3.6 1.2 4.2 3.5.7-2.1 2.7-3.6 4.9-3.5 2.9 0 4.5 2.2 4.5 6.2 0 1.9-.1 4.2-.1 6.5-.1.3.1.6.3.7l1.4 1.4v.1h-6.6zm-81.4-2l1.9 1.9v.1h-9.8v-.1l2-1.9c.2-.2.3-.4.3-.7V7.3c0-.5 0-1.2.1-1.8L11.4 22h-.1L4.5 6.8c-.1-.4-.2-.4-.3-.6v10c-.1.7 0 1.3.3 1.9l2.7 3.6v.1H0v-.1L2.7 18c.3-.6.4-1.3.3-1.9v-11c0-.5-.1-1.1-.5-1.5L.7 1.1V1h7l5.8 12.9L18.6 1h6.8v.1l-1.9 2.2c-.2.2-.3.5-.3.7v15.2c0 .2.1.5.3.6zm7.6-5.9c0 3.8 1.9 5.3 4.2 5.3 1.9.1 3.6-1 4.4-2.7h.1c-.8 3.7-3.1 5.5-6.5 5.5-3.7 0-7.2-2.2-7.2-7.4 0-5.5 3.5-7.6 7.3-7.6 3.1 0 6.4 1.5 6.4 6.2v.8h-8.7zm0-.8h4.3v-.8c0-3.9-.8-4.9-2-4.9-1.4.1-2.3 1.6-2.3 5.7z"></path></svg></div><div class="r bb"><svg width="35" height="35" viewBox="5 5 35 35" class="q"><path d="M5 40V5h35v35H5zm8.56-12.63c0 .56-.03.69-.32 1.03L10.8 31.4v.4h6.97v-.4L15.3 28.4c-.29-.34-.34-.5-.34-1.03v-8.95l6.13 13.36h.71l5.26-13.36v10.64c0 .3 0 .35-.19.53l-1.85 1.8v.4h9.2v-.4l-1.83-1.8c-.18-.18-.2-.24-.2-.53V15.94c0-.3.02-.35.2-.53l1.82-1.8v-.4h-6.47l-4.62 11.55-5.2-11.54h-6.8v.4l2.15 2.63c.24.3.29.37.29.77v10.35z"></path></svg></div></a><div class="ba al"><div class="jh" id="li-post-page-navbar-open-in-app-button"><div class="bd ba bb"><a href="https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F978f9e89ddaa&amp;~feature=LiMobileNavBar&amp;source=post_page-----978f9e89ddaa----------------------" class="be bf bg bh bi bj bk bl bm bn bo bp bq br bs bt" rel="noopener nofollow">Open in app</a></div></div></div><div class="ba g"><div class="bu bv r g"><svg width="2" height="29"><path d="M1 29V1" stroke="#D5D5D5" stroke-width="0.5" fill="none" stroke-linecap="round"></path></svg></div><div class="jh" id="li-post-page-navbar-topic-copy"><a href="https://medium.com/topic/?source=post_page-----978f9e89ddaa----------------------" rel="noopener"><div class="an ao ap aq ar as at au av aw ax ay az"></div></a></div></div></div></div><div class="r bw w"><span class="bx b by bz ca cb r cc cd"><div class="n o ce"><div class="n f"><div class="cl" aria-hidden="true"><div class="n"><button class="cu cv bg bh bi bj bk bl bm bn cw cx bq br cy cz"><svg width="25" height="25" viewBox="0 0 25 25" class="bd da r db dc"><path d="M20.07 18.93l-4.16-4.15a6 6 0 1 0-.88.88l4.15 4.16a.62.62 0 1 0 .89-.89zM6.5 11a4.75 4.75 0 1 1 9.5 0 4.75 4.75 0 0 1-9.5 0z"></path></svg></button><input class="cm cn co cp bz cq cr cs ct" placeholder="Search Medium" value=""></div></div></div><div class="ba dd"><a href="https://medium.com/search?source=post_page-----978f9e89ddaa----------------------" class="cu cv bg bh bi bj bk bl bm bn cw cx bq br cy cz" rel="noopener"><svg width="25" height="25" viewBox="0 0 25 25" class="bd de r db dc"><path d="M20.07 18.93l-4.16-4.15a6 6 0 1 0-.88.88l4.15 4.16a.62.62 0 1 0 .89-.89zM6.5 11a4.75 4.75 0 1 1 9.5 0 4.75 4.75 0 0 1-9.5 0z"></path></svg></a></div><a class="cu cv bg bh bi bj bk bl bm bn cw cx bq br cy cz" rel="noopener" href="https://medium.com/me/list/queue?source=post_page-----978f9e89ddaa----------------------"><svg width="25" height="25" viewBox="0 0 25 25" class="de r g"><path d="M16 6a2 2 0 0 1 2 2v13.66h-.01a.5.5 0 0 1-.12.29.5.5 0 0 1-.7.03l-5.67-4.13-5.66 4.13a.5.5 0 0 1-.7-.03.48.48 0 0 1-.13-.29H5V8c0-1.1.9-2 2-2h9zM6 8v12.64l5.16-3.67a.49.49 0 0 1 .68 0L17 20.64V8a1 1 0 0 0-1-1H7a1 1 0 0 0-1 1z"></path><path d="M21 5v13.66h-.01a.5.5 0 0 1-.12.29.5.5 0 0 1-.7.03l-.17-.12V5a1 1 0 0 0-1-1h-9a1 1 0 0 0-1 1H8c0-1.1.9-2 2-2h9a2 2 0 0 1 2 2z"></path></svg></a><div class="de n dc"><div class="cl" aria-hidden="true"><button class="cu cv bg bh bi bj bk bl bm bn cw cx bq br cy cz r"><svg width="25" height="25" viewBox="-293 409 25 25" class="df r"><path d="M-273.33 423.67l-1.67-1.52v-3.65a5.5 5.5 0 0 0-6.04-5.47 5.66 5.66 0 0 0-4.96 5.71v3.41l-1.68 1.55a1 1 0 0 0-.32.74V427a1 1 0 0 0 1 1h3.49a3.08 3.08 0 0 0 3.01 2.45 3.08 3.08 0 0 0 3.01-2.45h3.49a1 1 0 0 0 1-1v-2.59a1 1 0 0 0-.33-.74zm-7.17 5.63c-.84 0-1.55-.55-1.81-1.3h3.62a1.92 1.92 0 0 1-1.81 1.3zm6.35-2.45h-12.7v-2.35l1.63-1.5c.24-.22.37-.53.37-.85v-3.41a4.51 4.51 0 0 1 3.92-4.57 4.35 4.35 0 0 1 4.78 4.33v3.65c0 .32.14.63.38.85l1.62 1.48v2.37z"></path></svg></button></div></div><div class="jh" id="li-post-page-navbar-upsell-button"><div class="de r g"><div><a href="https://medium.com/membership?source=upgrade_membership---nav_full------------------------" class="dg ap q dh di dj dk dl bn cy dm dn do dp dq dr ds bx b by bz ca cb dt du dv cl dw bq" rel="noopener">Upgrade</a></div></div></div><div class="n" aria-hidden="true"><div class="dx dy n o"><button class="cu cv bg bh bi bj bk bl bm bn cw cx bq br cy cz"><img alt="Rixazeez" class="r dz ea eb" src="./good explanation of RL_files/0_Yf7A5FNFwvWBahQb.jpg" width="32" height="32"></button></div></div></div></span></div></div></div></div></div></nav><div class="ec aj r ed ak"></div><article><section class="ee ef eg eh ai ei dv n ej"></section><span class="r"></span><div><div class="s u ek el em en"></div><div class="eg eh ei cr"><div class="r h g f e"><aside class="qa s t" style="width: 295.5px;"><div class="qd qe s qf ck ai"><h4 class="bx fx gf bz cc"><span class="cl qe ck av cj">Top highlight</span></h4></div></aside></div></div><section class="eo ep eq er es"><div class="n p"><div class="z ab ac ae af et ah ai"><div><div id="6b57" class="eu ev ap by ew b ex ey ez fa fb fc fd fe ff fg fh"><h1 class="ew b ex fi ez fj fb fk fd fl ff fm ap"><strong class="bk">Deep Reinforcement Learning Demysitifed (Episode 2) ‚Äî Policy Iteration, Value Iteration and Q-learning</strong></h1></div><div class="fn"><div class="n fo fp fq fr"><div class="o n"><div><a rel="noopener" href="https://medium.com/@m.alzantot?source=post_page-----978f9e89ddaa----------------------"><img alt="Moustafa Alzantot" class="r dz fs ft" src="./good explanation of RL_files/0_vbsTKQhLiZPMFy9f_" width="48" height="48"></a></div><div class="fu ai r"><div class="n"><div style="flex:1"><span class="bx b by bz ca cb r ap q"><div class="fv n o fw"><span class="bx fx cp bz av fy cj as at au ap"><a class="cu cv bg bh bi bj bk bl bm bn fz bq br cy cz" rel="noopener" href="https://medium.com/@m.alzantot?source=post_page-----978f9e89ddaa----------------------">Moustafa Alzantot</a></span><div class="ga r bw h"><button class="gb dh be bf gc bo bp gd bn ds bx b by ge gf cb dt du dv cl dw bq">Follow</button></div></div></span></div></div><span class="bx b by bz ca cb r cc cd"><span class="bx fx cp bz av fy cj as at au cc"><div><a class="cu cv bg bh bi bj bk bl bm bn fz bq br cy cz" rel="noopener" href="https://medium.com/@m.alzantot/deep-reinforcement-learning-demysitifed-episode-2-policy-iteration-value-iteration-and-q-978f9e89ddaa?source=post_page-----978f9e89ddaa----------------------">Jul 8, 2017</a> <!-- -->¬∑<!-- --> <!-- -->11<!-- --> min read</div></span></span></div></div><div class="n gg gh gi gj gk gl gm gn y"><div class="n o"><div class="go r bw"><a href="https://medium.com/p/978f9e89ddaa/share/twitter?source=post_actions_header---------------------------" class="cu cv bg bh bi bj bk bl bm bn cw cx bq br cy cz" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="go r bw"><button class="cu cv bg bh bi bj bk bl bm bn cw cx bq br cy cz"><svg width="29" height="29" viewBox="0 0 29 29" fill="none" class="q"><path d="M5 6.36C5 5.61 5.63 5 6.4 5h16.2c.77 0 1.4.61 1.4 1.36v16.28c0 .75-.63 1.36-1.4 1.36H6.4c-.77 0-1.4-.6-1.4-1.36V6.36z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M10.76 20.9v-8.57H7.89v8.58h2.87zm-1.44-9.75c1 0 1.63-.65 1.63-1.48-.02-.84-.62-1.48-1.6-1.48-.99 0-1.63.64-1.63 1.48 0 .83.62 1.48 1.59 1.48h.01zM12.35 20.9h2.87v-4.79c0-.25.02-.5.1-.7.2-.5.67-1.04 1.46-1.04 1.04 0 1.46.8 1.46 1.95v4.59h2.87v-4.92c0-2.64-1.42-3.87-3.3-3.87-1.55 0-2.23.86-2.61 1.45h.02v-1.24h-2.87c.04.8 0 8.58 0 8.58z" fill="#fff"></path></svg></button></div><div class="go r bw"><a href="https://medium.com/p/978f9e89ddaa/share/facebook?source=post_actions_header---------------------------" class="cu cv bg bh bi bj bk bl bm bn cw cx bq br cy cz" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="gp r"><div><div class="gq"><div><div class="cl" role="tooltip" aria-hidden="true" aria-describedby="1" aria-labelledby="1"><button class="cu cv bg bh bi bj bk bl bm bn cw cx bq br cy cz"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div><div class="gr r am"><div class="cl" aria-hidden="true"><div class="r bw"><button class="cu cv bg bh bi bj bk bl bm bn cw cx bq br cy cz"><svg width="25" height="25" viewBox="-480.5 272.5 21 21" class="q"><path d="M-463 284.6c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5z"></path></svg></button></div></div></div></div></div></div></div></div><p id="4179" class="gs gt ap by gu b gv gw gx gy gz ha hb hc hd he hf eo" data-selectable-paragraph="">In previous two articles, we introduced reinforcement learning definition, examples, and simple solving strategies using <a class="cu dw hg hh hi hj" target="_blank" rel="noopener" href="https://medium.com/@m.alzantot/deep-reinforcement-learning-demystified-episode-0-2198c05a6124">random policy search</a> and <a href="https://becominghuman.ai/genetic-algorithm-for-reinforcement-learning-a38a5612c4dc" class="cu dw hg hh hi hj" target="_blank" rel="noopener nofollow">genetic algorithms</a>.</p><p id="9db7" class="gs gt ap by gu b gv gw gx gy gz ha hb hc hd he hf eo" data-selectable-paragraph="">In practice, random search does not work well for complex problems where the search space (that depends on the number of possible states and actions) is large. Also, genetic algorithm is a meta-heuristic optimization so it does not provide a guarantee to find an optimal solution. In this article, we are going to introduce fundamental reinforcement learning algorithms.</p><p id="4528" class="gs gt ap by gu b gv gw gx gy gz ha hb hc hd he hf eo" data-selectable-paragraph="">We start by reviewing the Markov Decision Process formulation, then we describe the value-iteration and policy iteration which are algorithms for finding the optimal policy when the agent knows sufficient details about the environment model. We then, describe the Q-learning is a model-free learning that can be used when the agent does not know the environment model but has to discover the policy by trial and error making use of its history of interaction with the environment. We also provide demonstration examples of the three methods by using the <a href="https://gym.openai.com/envs/FrozenLake8x8-v0" class="cu dw hg hh hi hj" target="_blank" rel="noopener nofollow">FrozenLake8x8</a> and <a href="https://gym.openai.com/envs/MountainCar-v0" class="cu dw hg hh hi hj" target="_blank" rel="noopener nofollow">MountainCar</a> problems from OpenAI gym.</p><h2 id="9611" class="hk hl ap by bx hm hn ho hp hq hr hs ht hu hv hw hx" data-selectable-paragraph="">Markov Decision Process (MDP)</h2><p id="692e" class="gs gt ap by gu b gv hy gx hz gz ia hb ib hd ic hf eo" data-selectable-paragraph="">We briefly introduced Markov Decision Process <code class="id ie if ig ih b"><strong class="gu ii">MDP</strong></code>in our first article. To recall, in reinforcement learning problems we have an agent interacting with an environment. At each time step, the agent performs an action which leads to two things: changing the environment state and the agent (possibly) receiving a reward (<em class="ij">or penalty</em>) from the environment. The goal of the agent is to discover an optimal policy (i.e. what actions to do in each state) such that it maximizes the total value of rewards received from the environment in response to its actions. <code class="id ie if ig ih b"><strong class="gu ii">MDP</strong></code>is<strong class="gu ii"> </strong>used to describe the agent/ environment interaction settings in a formal way.</p><p id="7851" class="gs gt ap by gu b gv gw gx gy gz ha hb hc hd he hf eo" data-selectable-paragraph=""><strong class="gu ii">MDP</strong> consists of a tuple of 5 elements:</p><ul class=""><li id="d8b8" class="gs gt ap by gu b gv gw gx gy gz ha hb hc hd he hf ik il im" data-selectable-paragraph=""><code class="id ie if ig ih b"><strong class="gu ii">S</strong></code> : Set of states. At each time step the state of the environment is an element <code class="id ie if ig ih b">s ‚àà <strong class="gu ii">S</strong></code>.</li><li id="273f" class="gs gt ap by gu b gv in gx io gz ip hb iq hd ir hf ik il im" data-selectable-paragraph=""><code class="id ie if ig ih b"><strong class="gu ii">A</strong></code>: Set of actions. At each time step the agent choses an action <code class="id ie if ig ih b">a ‚àà <strong class="gu ii">A</strong></code> to perform.</li><li id="f62f" class="gs gt ap by gu b gv in gx io gz ip hb iq hd ir hf ik il im" data-selectable-paragraph=""><code class="id ie if ig ih b"><strong class="gu ii">p(s_{t+1} | s_t, a_t)</strong></code> : State transition model that describes how the environment state changes when the user performs an action <code class="id ie if ig ih b">a</code> depending on the action <code class="id ie if ig ih b">a</code>and the current state <strong class="gu ii">s</strong>.</li><li id="bc62" class="gs gt ap by gu b gv in gx io gz ip hb iq hd ir hf ik il im" data-selectable-paragraph=""><code class="id ie if ig ih b"><strong class="gu ii">p(r_{t+1} | s_t, a_t)</strong></code> : Reward model that describes the real-valued reward value that the agent receives from the environment after performing an action. In MDP the the reward value depends on the current state and the action performed.</li><li id="e936" class="gs gt ap by gu b gv in gx io gz ip hb iq hd ir hf ik il im" data-selectable-paragraph=""><strong class="gu ii">ùõæ</strong> : discount factor that controls the importance of future rewards. We will describe it in more details later.</li></ul><p id="4cd1" class="gs gt ap by gu b gv gw gx gy gz ha hb hc hd he hf eo" data-selectable-paragraph="">The way by which the agent chooses which action to perform is named the agent <code class="id ie if ig ih b">policy</code> which is a function that takes the current environment state to return an action. The policy is often denoted by the symbol ùõë.</p><figure class="it iu iv iw ix iy eg eh paragraph-image"><div class="eg eh is"><div class="jd r cr id"><div class="je r"><div class="cq iz s t u ja ai av jb jc"><img class="s t u ja ai jf jg bc ql" src="./good explanation of RL_files/1_nS-MKI7pY8PeQLDCQ64xCA@2x.png" width="394" height="74" role="presentation"></div><img class="od pz s t u ja ai ji" width="394" height="74" srcset="" sizes="394px" role="presentation" src="./good explanation of RL_files/1_nS-MKI7pY8PeQLDCQ64xCA@2x(1).png"><noscript><img class="s t u ja ai" src="https://miro.medium.com/max/788/1*nS-MKI7pY8PeQLDCQ64xCA@2x.png" width="394" height="74" role="presentation"/></noscript></div></div></div></figure><p id="9956" class="gs gt ap by gu b gv gw gx gy gz ha hb hc hd he hf eo" data-selectable-paragraph="">Let‚Äôs now differentiate between two types environments.</p><p id="7ca2" class="gs gt ap by gu b gv gw gx gy gz ha hb hc hd he hf eo" data-selectable-paragraph=""><strong class="gu ii">Deterministic environment</strong>: deterministic environment means that both state transition model and reward model are deterministic functions. If the agent while in a given state repeats a given action, it will always go the same next state and receive the same reward value.</p><p id="7d25" class="gs gt ap by gu b gv gw gx gy gz ha hb hc hd he hf eo" data-selectable-paragraph=""><strong class="gu ii">Stochastic environment</strong>: In a stochastic environment there is uncertainty about the actions effect. When the agent repeats doing the same action in a given state, the new state and received reward may not be the same each time. For example, a robot which tries to move forward but because of the imperfection in the robot operation or other factors in the environment (e.g. slippery floor), sometimes the action <code class="id ie if ig ih b">forward</code> will make it move forward but in sometimes, it will move to <code class="id ie if ig ih b">left</code> or <code class="id ie if ig ih b">right.</code></p><p id="59f8" class="gs gt ap by gu b gv gw gx gy gz ha hb hc hd he hf eo" data-selectable-paragraph="">Deterministic environments are easier to solve, because the agent knows how to plan its actions with no-uncertainty given the environment MDP. Possibly, the environment can be modeled in as a graph where each state is a node and edges represent transition actions from one state to another and edge weights are received rewards. Then, the agent can use a graph search algorithm such as A* to find the path with maximum total reward form the initial state.</p><h2 id="b6d4" class="hk hl ap by bx hm hn ho hp hq hr hs ht hu hv hw hx" data-selectable-paragraph="">Total reward</h2><p id="eae3" class="gs gt ap by gu b gv hy gx hz gz ia hb ib hd ic hf eo" data-selectable-paragraph="">Remember, that the goal of the agent is to pick the best policy that will maximize the total rewards received from the environment.</p><p id="0ce8" class="gs gt ap by gu b gv gw gx gy gz ha hb hc hd he hf eo" data-selectable-paragraph="">Assume that environment is initially at state <code class="id ie if ig ih b">s_0</code></p><p id="8366" class="gs gt ap by gu b gv gw gx gy gz ha hb hc hd he hf eo" data-selectable-paragraph=""><strong class="gu ii">At time </strong><code class="id ie if ig ih b"><strong class="gu ii">0</strong></code> : Agent observes the environment state <code class="id ie if ig ih b">s_0</code> and picks an action <code class="id ie if ig ih b">a_0</code>, then upon performing its action, environment state becomes <code class="id ie if ig ih b">s_1</code> and the agent receives a reward <code class="id ie if ig ih b">r_1</code> .</p><p id="235d" class="gs gt ap by gu b gv gw gx gy gz ha hb hc hd he hf eo" data-selectable-paragraph=""><strong class="gu ii">At time </strong><code class="id ie if ig ih b"><strong class="gu ii">1</strong></code>: Agent observes current state <code class="id ie if ig ih b">s_1</code> and picks an action <code class="id ie if ig ih b">a_1</code> , then upon acting its action, environment state becomes <code class="id ie if ig ih b">s_2</code> and it receives a reward <code class="id ie if ig ih b">r_2</code> .</p><p id="f3ac" class="gs gt ap by gu b gv gw gx gy gz ha hb hc hd he hf eo" data-selectable-paragraph=""><strong class="gu ii">At time </strong><code class="id ie if ig ih b"><strong class="gu ii">2</strong></code>: Agent observes current state <code class="id ie if ig ih b">s_2</code> and picks an action <code class="id ie if ig ih b">a_2</code> , then upon acting its action, environment state becomes <code class="id ie if ig ih b">s_3</code> and it receives a reward <code class="id ie if ig ih b">r_3</code> .</p><p id="e6ed" class="gs gt ap by gu b gv gw gx gy gz ha hb hc hd he hf eo" data-selectable-paragraph="">So the total reward received by the agent in response to the actions selected by its policy is going to be:</p><p id="c2e2" class="gs gt ap by gu b gv gw gx gy gz ha hb hc hd he hf eo" data-selectable-paragraph="">Total reward = r_1 + r_2 + r_3 + r_4 + r_5 + ..</p><p id="d4ff" class="gs gt ap by gu b gv gw gx gy gz ha hb hc hd he hf eo" data-selectable-paragraph="">However, it is common to use a discount factor to give higher weight to near rewards received near than rewards received further in the future.</p><p id="f4f8" class="gs gt ap by gu b gv gw gx gy gz ha hb hc hd he hf eo" data-selectable-paragraph="">Total discounted reward = r_1 +<strong class="gu ii"> ùõæ r_2 + ùõæ¬≤ r_3 + ùõæ¬≥ r_4 + ùõæ‚Å¥ r_5+ ‚Ä¶<br></strong>so,</p><figure class="it iu iv iw ix iy eg eh paragraph-image"><div class="jk jl cr jm ai"><div class="eg eh jj"><div class="jd r cr id"><div class="jn r"><div class="cq iz s t u ja ai av jb jc"><img class="s t u ja ai jf jg bc ql" src="./good explanation of RL_files/1_Jix2ScBmffb1e5MpZCiwMg@2x.png" width="2336" height="424" role="presentation"></div><img class="od pz s t u ja ai ji" width="2336" height="424" srcset="https://miro.medium.com/max/1104/1*Jix2ScBmffb1e5MpZCiwMg@2x.png 552w, https://miro.medium.com/max/1400/1*Jix2ScBmffb1e5MpZCiwMg@2x.png 700w" sizes="700px" role="presentation" src="./good explanation of RL_files/1_Jix2ScBmffb1e5MpZCiwMg@2x(1).png"><noscript><img class="s t u ja ai" src="https://miro.medium.com/max/4672/1*Jix2ScBmffb1e5MpZCiwMg@2x.png" width="2336" height="424" srcSet="https://miro.medium.com/max/1104/1*Jix2ScBmffb1e5MpZCiwMg@2x.png 552w, https://miro.medium.com/max/1400/1*Jix2ScBmffb1e5MpZCiwMg@2x.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="caa6" class="gs gt ap by gu b gv gw gx gy gz ha hb hc hd he hf eo" data-selectable-paragraph="">where <code class="id ie if ig ih b">T</code> is the horizon (episode length) which can be infinity if there is maximum length for the episode.</p><p id="6651" class="gs gt ap by gu b gv gw gx gy gz ha hb hc hd he hf eo" data-selectable-paragraph="">The reason for using discount factor is to prevent the total reward from going to infinity (because <code class="id ie if ig ih b">0 ‚â§ ùõæ ‚â§ 1</code>), it also models the agent behavior when the agent prefers immediate rewards than rewards that are potentially received far away in the future. (If I give you 1000 dollars today and If I give you 1000 days after 10 years which one would you prefer ? ). Imagine a robot that is trying to solve a maze and there are two paths to the goal state one of them is longer but gives higher reward while there is a shorter path with smaller reward. By adjusting the <code class="id ie if ig ih b">ùõæ</code> value, you can control which the path the agent should prefer.</p><p id="c3e5" class="gs gt ap by gu b gv gw gx gy gz ha hb hc hd he hf eo" data-selectable-paragraph="">Now, we are ready to introduce the <strong class="gu ii">value-iteration</strong> and <strong class="gu ii">policy-iteration</strong> algorithms. These are two fundamental methods for solving MDPs. Both value-iteration and policy-iteration assume that the agent knows the MDP model of the world (i.e. the agent knows the state-transition and reward probability functions). Therefore, they can be used by the agent to (<em class="ij">offline</em>) plan its actions given knowledge about the environment before interacting with it. Later, we will discuss <strong class="gu ii">Q-learning</strong> which is a model-free learning environment that can be used in situation where the agent initially knows only that are the possible states and actions but doesn't know the state-transition and reward probability functions. In Q-learning the agent improves its behavior (<em class="ij">online</em>) through learning from the history of interactions with the environment.</p><h2 id="c2d8" class="hk hl ap by bx hm hn ho hp hq hr hs ht hu hv hw hx" data-selectable-paragraph="">Value function</h2><p id="71bc" class="gs gt ap by gu b gv hy gx hz gz ia hb ib hd ic hf eo" data-selectable-paragraph="">Many reinforcement learning introduce the notion of `<strong class="gu ii">value-function</strong>` which often denoted as <code class="id ie if ig ih b">V(s)</code> . The value function represent how good is a state for an agent to be in. It is equal to expected total reward for an agent starting from state <code class="id ie if ig ih b">s</code>. The value function depends on the policy by which the agent picks actions to perform. So, if the agent uses a given policy ùõë to select actions, the corresponding value function is given by:</p><figure class="it iu iv iw ix iy eg eh paragraph-image"><div class="jk jl cr jm ai"><div class="eg eh jo"><div class="jd r cr id"><div class="jp r"><div class="cq iz s t u ja ai av jb jc"><img class="s t u ja ai jf jg bc ql" src="./good explanation of RL_files/1_V6MR4fgJnG1Sk-g8_k_Wug@2x.png" width="1896" height="424" role="presentation"></div><img class="od pz s t u ja ai ji" width="1896" height="424" srcset="https://miro.medium.com/max/1104/1*V6MR4fgJnG1Sk-g8_k_Wug@2x.png 552w, https://miro.medium.com/max/1400/1*V6MR4fgJnG1Sk-g8_k_Wug@2x.png 700w" sizes="700px" role="presentation" src="./good explanation of RL_files/1_V6MR4fgJnG1Sk-g8_k_Wug@2x(1).png"><noscript><img class="s t u ja ai" src="https://miro.medium.com/max/3792/1*V6MR4fgJnG1Sk-g8_k_Wug@2x.png" width="1896" height="424" srcSet="https://miro.medium.com/max/1104/1*V6MR4fgJnG1Sk-g8_k_Wug@2x.png 552w, https://miro.medium.com/max/1400/1*V6MR4fgJnG1Sk-g8_k_Wug@2x.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="9715" class="gs gt ap by gu b gv gw gx gy gz ha hb hc hd he hf eo" data-selectable-paragraph="">Among all possible value-functions, there exist an <strong class="gu ii">optimal value function </strong>that has higher value than other functions for all states.</p><figure class="it iu iv iw ix iy eg eh paragraph-image"><div class="jk jl cr jm ai"><div class="eg eh jq"><div class="jd r cr id"><div class="jr r"><div class="cq iz s t u ja ai av jb jc"><img class="s t u ja ai jf jg bc ql" src="./good explanation of RL_files/1_MMJt0UaDc_GrpY61Wb1dXw@2x.png" width="1752" height="200" role="presentation"></div><img class="od pz s t u ja ai ji" width="1752" height="200" srcset="https://miro.medium.com/max/1104/1*MMJt0UaDc_GrpY61Wb1dXw@2x.png 552w, https://miro.medium.com/max/1400/1*MMJt0UaDc_GrpY61Wb1dXw@2x.png 700w" sizes="700px" role="presentation" src="./good explanation of RL_files/1_MMJt0UaDc_GrpY61Wb1dXw@2x(1).png"><noscript><img class="s t u ja ai" src="https://miro.medium.com/max/3504/1*MMJt0UaDc_GrpY61Wb1dXw@2x.png" width="1752" height="200" srcSet="https://miro.medium.com/max/1104/1*MMJt0UaDc_GrpY61Wb1dXw@2x.png 552w, https://miro.medium.com/max/1400/1*MMJt0UaDc_GrpY61Wb1dXw@2x.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="1332" class="gs gt ap by gu b gv gw gx gy gz ha hb hc hd he hf eo" data-selectable-paragraph="">The optimal policy ùõë* is the policy that corresponds to optimal value function.</p><figure class="it iu iv iw ix iy eg eh paragraph-image"><div class="jk jl cr jm ai"><div class="eg eh js"><div class="jd r cr id"><div class="jt r"><div class="cq iz s t u ja ai av jb jc"><img class="s t u ja ai jf jg bc ql" src="./good explanation of RL_files/1_pO1aahVf3P8KLe5djev1jQ@2x.png" width="1796" height="200" role="presentation"></div><img class="od pz s t u ja ai ji" width="1796" height="200" srcset="https://miro.medium.com/max/1104/1*pO1aahVf3P8KLe5djev1jQ@2x.png 552w, https://miro.medium.com/max/1400/1*pO1aahVf3P8KLe5djev1jQ@2x.png 700w" sizes="700px" role="presentation" src="./good explanation of RL_files/1_pO1aahVf3P8KLe5djev1jQ@2x(1).png"><noscript><img class="s t u ja ai" src="https://miro.medium.com/max/3592/1*pO1aahVf3P8KLe5djev1jQ@2x.png" width="1796" height="200" srcSet="https://miro.medium.com/max/1104/1*pO1aahVf3P8KLe5djev1jQ@2x.png 552w, https://miro.medium.com/max/1400/1*pO1aahVf3P8KLe5djev1jQ@2x.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="1170" class="gs gt ap by gu b gv gw gx gy gz ha hb hc hd he hf eo" data-selectable-paragraph="">In addition to the state value-function, for convenience RL algorithms introduce another function which is the state-action pair <strong class="gu ii">Q function</strong>. Q is a function of a state-action pair and returns a real value.</p><figure class="it iu iv iw ix iy eg eh paragraph-image"><div class="jk jl cr jm ai"><div class="eg eh ju"><div class="jd r cr id"><div class="jv r"><div class="cq iz s t u ja ai av jb jc"><img class="s t u ja ai jf jg bc ql" src="./good explanation of RL_files/1_B4kn7-Tb7AJ4myyeBvpHcw@2x.png" width="912" height="132" role="presentation"></div><img class="od pz s t u ja ai ji" width="912" height="132" srcset="https://miro.medium.com/max/1104/1*B4kn7-Tb7AJ4myyeBvpHcw@2x.png 552w, https://miro.medium.com/max/1400/1*B4kn7-Tb7AJ4myyeBvpHcw@2x.png 700w" sizes="700px" role="presentation" src="./good explanation of RL_files/1_B4kn7-Tb7AJ4myyeBvpHcw@2x(1).png"><noscript><img class="s t u ja ai" src="https://miro.medium.com/max/1824/1*B4kn7-Tb7AJ4myyeBvpHcw@2x.png" width="912" height="132" srcSet="https://miro.medium.com/max/1104/1*B4kn7-Tb7AJ4myyeBvpHcw@2x.png 552w, https://miro.medium.com/max/1400/1*B4kn7-Tb7AJ4myyeBvpHcw@2x.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="e6a9" class="gs gt ap by gu b gv gw gx gy gz ha hb hc hd he hf eo" data-selectable-paragraph="">The optimal Q-function <code class="id ie if ig ih b"><strong class="gu ii">Q*(s, a)</strong></code> means the expected total reward received by an agent starting in <code class="id ie if ig ih b"><strong class="gu ii">s</strong></code>and picks action <code class="id ie if ig ih b"><strong class="gu ii">a</strong></code>, then will behave optimally afterwards. <mark class="qb qc ls">There, </mark><mark class="qb qc ls"><strong class="gu ii">Q*(s, a)</strong></mark><mark class="qb qc ls"> is an indication for how good it is for an agent to pick action a while being in state s.</mark></p><p id="b047" class="gs gt ap by gu b gv gw gx gy gz ha hb hc hd he hf eo" data-selectable-paragraph="">Since <code class="id ie if ig ih b">V*(s)</code> is the maximum expected total reward when starting from state <strong class="gu ii">s</strong> , it will be the maximum of <code class="id ie if ig ih b">Q*(s, a)</code>over all possible actions.</p><p id="29dc" class="gs gt ap by gu b gv gw gx gy gz ha hb hc hd he hf eo" data-selectable-paragraph="">Therefore, the relationship between Q*(s, a) and V*(s) is easily obtained as:</p><figure class="it iu iv iw ix iy eg eh paragraph-image"><div class="jk jl cr jm ai"><div class="eg eh jw"><div class="jd r cr id"><div class="jx r"><div class="cq iz s t u ja ai av jb jc"><img class="s t u ja ai jf jg bc ql" src="./good explanation of RL_files/1_mLQA-JCUiGRYs8H-O0VBKA@2x.png" width="1876" height="200" role="presentation"></div><img class="od pz s t u ja ai ji" width="1876" height="200" srcset="https://miro.medium.com/max/1104/1*mLQA-JCUiGRYs8H-O0VBKA@2x.png 552w, https://miro.medium.com/max/1400/1*mLQA-JCUiGRYs8H-O0VBKA@2x.png 700w" sizes="700px" role="presentation" src="./good explanation of RL_files/1_mLQA-JCUiGRYs8H-O0VBKA@2x(1).png"><noscript><img class="s t u ja ai" src="https://miro.medium.com/max/3752/1*mLQA-JCUiGRYs8H-O0VBKA@2x.png" width="1876" height="200" srcSet="https://miro.medium.com/max/1104/1*mLQA-JCUiGRYs8H-O0VBKA@2x.png 552w, https://miro.medium.com/max/1400/1*mLQA-JCUiGRYs8H-O0VBKA@2x.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="9af5" class="gs gt ap by gu b gv gw gx gy gz ha hb hc hd he hf eo" data-selectable-paragraph="">and If we know the optimal Q-function <code class="id ie if ig ih b">Q*(s, a)</code> , the optimal policy can be easily extracted by choosing the action <code class="id ie if ig ih b">a </code>that gives maximum <strong class="gu ii">Q*(s, a) </strong>for state <code class="id ie if ig ih b">s</code>.</p><figure class="it iu iv iw ix iy eg eh paragraph-image"><div class="jk jl cr jm ai"><div class="eg eh jy"><div class="jd r cr id"><div class="jz r"><div class="cq iz s t u ja ai av jb jc"><img class="s t u ja ai jf jg bc ql" src="./good explanation of RL_files/1_YdSWi5JeQ83FWQeKk013-A@2x.png" width="2104" height="200" role="presentation"></div><img class="od pz s t u ja ai ji" width="2104" height="200" srcset="https://miro.medium.com/max/1104/1*YdSWi5JeQ83FWQeKk013-A@2x.png 552w, https://miro.medium.com/max/1400/1*YdSWi5JeQ83FWQeKk013-A@2x.png 700w" sizes="700px" role="presentation" src="./good explanation of RL_files/1_YdSWi5JeQ83FWQeKk013-A@2x(1).png"><noscript><img class="s t u ja ai" src="https://miro.medium.com/max/4208/1*YdSWi5JeQ83FWQeKk013-A@2x.png" width="2104" height="200" srcSet="https://miro.medium.com/max/1104/1*YdSWi5JeQ83FWQeKk013-A@2x.png 552w, https://miro.medium.com/max/1400/1*YdSWi5JeQ83FWQeKk013-A@2x.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="ed64" class="gs gt ap by gu b gv gw gx gy gz ha hb hc hd he hf eo" data-selectable-paragraph="">Now, lets introduce an important equation called the<strong class="gu ii"> </strong><a href="https://en.wikipedia.org/wiki/Bellman_equation" class="cu dw hg hh hi hj" target="_blank" rel="noopener nofollow"><strong class="gu ii">Bellman equation</strong></a> which is a super-important equation optimization and have applications in many fields such as reinforcement learning, economics and control theory. Bellman equation using dynamic programming paradigm provides a recursive definition for the optimal Q-function.<br>The <code class="id ie if ig ih b">Q*(s, a)</code> is equal to the summation of immediate reward after performing action <code class="id ie if ig ih b">a</code> while in state <code class="id ie if ig ih b">s</code> and the discounted expected future reward after transition to a next state <code class="id ie if ig ih b">s'.</code></p><figure class="it iu iv iw ix iy eg eh paragraph-image"><div class="jk jl cr jm ai"><div class="eg eh ka"><div class="jd r cr id"><div class="kb r"><div class="cq iz s t u ja ai av jb jc"><img class="s t u ja ai jf jg bc ql" src="./good explanation of RL_files/1_mAjQD3MsGTHGPBcBTf1Lyg@2x.png" width="2920" height="1348" role="presentation"></div><img class="od pz s t u ja ai ji" width="2920" height="1348" srcset="https://miro.medium.com/max/1104/1*mAjQD3MsGTHGPBcBTf1Lyg@2x.png 552w, https://miro.medium.com/max/1400/1*mAjQD3MsGTHGPBcBTf1Lyg@2x.png 700w" sizes="700px" role="presentation" src="./good explanation of RL_files/1_mAjQD3MsGTHGPBcBTf1Lyg@2x(1).png"><noscript><img class="s t u ja ai" src="https://miro.medium.com/max/5840/1*mAjQD3MsGTHGPBcBTf1Lyg@2x.png" width="2920" height="1348" srcSet="https://miro.medium.com/max/1104/1*mAjQD3MsGTHGPBcBTf1Lyg@2x.png 552w, https://miro.medium.com/max/1400/1*mAjQD3MsGTHGPBcBTf1Lyg@2x.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="5c61" class="gs gt ap by gu b gv gw gx gy gz ha hb hc hd he hf eo" data-selectable-paragraph="">Value-iteration and policy iteration rely on these equations to compute the optimal value-function.</p><h1 id="297e" class="kc hl ap by bx hm ex kd ez ke kf kg kh ki kj kk kl" data-selectable-paragraph=""><strong class="bk">Value Iteration</strong></h1><p id="9ae8" class="gs gt ap by gu b gv hy gx hz gz ia hb ib hd ic hf eo" data-selectable-paragraph="">Value iteration computes the optimal state value function by iteratively improving the estimate of <strong class="gu ii">V(s)</strong>. The algorithm initialize <strong class="gu ii">V(s)</strong> to arbitrary random values. It repeatedly updates the <strong class="gu ii">Q(s, a)</strong> and <strong class="gu ii">V(s)</strong> values until they converges. Value iteration is guaranteed to converge to the optimal values. This algorithm is shown in the following pseudo-code:</p><figure class="it iu iv iw ix iy eg eh paragraph-image"><div class="jk jl cr jm ai"><div class="eg eh km"><div class="jd r cr id"><div class="kn r"><div class="cq iz s t u ja ai av jb jc"><img class="s t u ja ai jf jg bc ql" src="./good explanation of RL_files/1_MsD6og8hCReDO24T8iZfNw.png" width="856" height="334" role="presentation"></div><img class="od pz s t u ja ai ji" width="856" height="334" srcset="https://miro.medium.com/max/1104/1*MsD6og8hCReDO24T8iZfNw.png 552w, https://miro.medium.com/max/1400/1*MsD6og8hCReDO24T8iZfNw.png 700w" sizes="700px" role="presentation" src="https://miro.medium.com/max/856/1*MsD6og8hCReDO24T8iZfNw.png"><noscript><img class="s t u ja ai" src="https://miro.medium.com/max/1712/1*MsD6og8hCReDO24T8iZfNw.png" width="856" height="334" srcSet="https://miro.medium.com/max/1104/1*MsD6og8hCReDO24T8iZfNw.png 552w, https://miro.medium.com/max/1400/1*MsD6og8hCReDO24T8iZfNw.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div><figcaption class="cc cp ko kp kq ei eg eh kr ks bx fx" data-selectable-paragraph=""><strong class="bx ii">Pseudo code for value-iteration algorithm. Credit: Alpaydin Introduction to Machine Learning, 3rd edition.</strong></figcaption></figure><h2 id="d6f4" class="hk hl ap by bx hm hn ho hp hq hr hs ht hu hv hw hx" data-selectable-paragraph="">Example : FrozenLake8x8 (Using Value-Iteration)</h2><p id="f378" class="gs gt ap by gu b gv hy gx hz gz ia hb ib hd ic hf eo" data-selectable-paragraph="">Now lets implement it in python to solve the <a href="https://gym.openai.com/envs/FrozenLake8x8-v0" class="cu dw hg hh hi hj" target="_blank" rel="noopener nofollow">FrozenLake8x8</a> openAI gym. compared to the FrozenLake-v0 environment we solved earlier using genetic algorithm, the <a href="https://gym.openai.com/envs/FrozenLake8x8-v0" class="cu dw hg hh hi hj" target="_blank" rel="noopener nofollow">FrozenLake8x8</a> has 64 possible states (grid size is 8x8) instead of 16. Therefore, the problem becomes harder and genetic algorithm will struggle to find the optimal solution.</p><figure class="it iu iv iw ix iy"><div class="jd r cr"><div class="qh r"><iframe src="./good explanation of RL_files/b0f0ac0bfbf315937689bc3c25944d53.html" allowfullscreen="" frameborder="0" height="1948" width="680" title="Solution of FrozenLake8x8 environment using Value Iteration." class="s t u ja ai" scrolling="auto"></iframe></div></div><figcaption class="cc cp ko kp kq ei eg eh kr ks bx fx">Solution of the FrozenLake8x8 environment using Value-Iteration</figcaption></figure><p id="8033" class="gs gt ap by gu b gv gw gx gy gz ha hb hc hd he hf eo" data-selectable-paragraph=""><strong class="gu ii">Policy Iteration</strong></p><p id="8992" class="gs gt ap by gu b gv gw gx gy gz ha hb hc hd he hf eo" data-selectable-paragraph="">While value-iteration algorithm keeps improving the value function at each iteration until the value-function converges. Since the agent only cares about the finding the optimal policy, sometimes the optimal policy will converge before the value function. Therefore, another algorithm called policy-iteration instead of repeated improving the value-function estimate, it will re-define the policy at each step and compute the value according to this new policy until the policy converges. Policy iteration is also guaranteed to converge to the optimal policy and it often takes less iterations to converge than the value-iteration algorithm.</p><p id="a815" class="gs gt ap by gu b gv gw gx gy gz ha hb hc hd he hf eo" data-selectable-paragraph="">The pseudo code for Policy Iteration is shown below.</p><figure class="it iu iv iw ix iy eg eh paragraph-image"><div class="jk jl cr jm ai"><div class="eg eh ku"><div class="jd r cr id"><div class="kv r"><div class="od pz s t u ja ai av jb jc"><img class="s t u ja ai jf jg jh" src="./good explanation of RL_files/1_WwOaLxFvDDgY0Uk92FO6Rw.png" width="974" height="402" role="presentation"></div><img class="cq iz s t u ja ai ji" width="974" height="402" srcset="https://miro.medium.com/max/1104/1*WwOaLxFvDDgY0Uk92FO6Rw.png 552w, https://miro.medium.com/max/1400/1*WwOaLxFvDDgY0Uk92FO6Rw.png 700w" sizes="700px" role="presentation"><noscript><img class="s t u ja ai" src="https://miro.medium.com/max/1948/1*WwOaLxFvDDgY0Uk92FO6Rw.png" width="974" height="402" srcSet="https://miro.medium.com/max/1104/1*WwOaLxFvDDgY0Uk92FO6Rw.png 552w, https://miro.medium.com/max/1400/1*WwOaLxFvDDgY0Uk92FO6Rw.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div><figcaption class="cc cp ko kp kq ei eg eh kr ks bx fx" data-selectable-paragraph=""><strong class="bx ii">Pseudo code for policy-iteration algorithm. Credit: Alpaydin Introduction to Machine Learning, 3rd edition.</strong></figcaption></figure><h2 id="cc19" class="hk hl ap by bx hm hn ho hp hq hr hs ht hu hv hw hx" data-selectable-paragraph="">Example : FrozenLake8x8 (Using Policy-Iteration)</h2><figure class="it iu iv iw ix iy"><div class="jd r cr"><div class="qj r"><iframe src="./good explanation of RL_files/2ca8bcdc117cdb6e07013a590b3e78b4.html" allowfullscreen="" frameborder="0" height="1733" width="680" title="frozenlake8x8_policyiteration.py" class="s t u ja ai" scrolling="auto"></iframe></div></div><figcaption class="cc cp ko kp kq ei eg eh kr ks bx fx">Solution of the FrozenLake8x8 environment using Policy Iteration</figcaption></figure><h2 id="a980" class="hk hl ap by bx hm hn ho hp hq hr hs ht hu hv hw hx" data-selectable-paragraph="">Value-Iteration vs Policy-Iteration</h2><p id="4ab3" class="gs gt ap by gu b gv hy gx hz gz ia hb ib hd ic hf eo" data-selectable-paragraph="">Both value-iteration and policy-iteration algorithms can be used for <em class="ij">offline planning</em> where the agent is assumed to have prior knowledge about the effects of its actions on the environment (they assume the MDP model is known). Comparing to each other, policy-iteration is computationally efficient as it often takes considerably fewer number of iterations to converge although each iteration is more computationally expensive.</p><h1 id="41f9" class="kc hl ap by bx hm ex kd ez ke kf kg kh ki kj kk kl" data-selectable-paragraph="">Q-Learning</h1><p id="bc24" class="gs gt ap by gu b gv hy gx hz gz ia hb ib hd ic hf eo" data-selectable-paragraph="">Now, lets consider the case where the agent does not know apriori what are the effects of its actions on the environment (state transition and reward models are not known). The agent only knows what are the set of possible states and actions, and can observe the environment current state. In this case, the agent has to actively learn through the experience of interactions with the environment. There are two categories of learning algorithms:</p><p id="a8fe" class="gs gt ap by gu b gv gw gx gy gz ha hb hc hd he hf eo" data-selectable-paragraph=""><strong class="gu ii">model-based learning: </strong>In model-based learning, the agent will interact to the environment and from the history of its interactions, the agent will try to approximate the environment state transition and reward models. Afterwards, given the models it learnt, the agent can use value-iteration or policy-iteration to find an optimal policy.</p><p id="70a0" class="gs gt ap by gu b gv gw gx gy gz ha hb hc hd he hf eo" data-selectable-paragraph=""><strong class="gu ii">model-free learning:</strong> in model-free learning, the agent will not try to learn explicit models of the environment state transition and reward functions. However, it directly derives an optimal policy from the interactions with the environment.</p><p id="a24b" class="gs gt ap by gu b gv gw gx gy gz ha hb hc hd he hf eo" data-selectable-paragraph=""><strong class="gu ii">Q-Learning</strong> is an example of model-free learning algorithm. It does not assume that agent knows anything about the state-transition and reward models. However, the agent will discover what are the good and bad actions by trial and error.</p><p id="4efa" class="gs gt ap by gu b gv gw gx gy gz ha hb hc hd he hf eo" data-selectable-paragraph="">The basic idea of Q-Learning is to approximate the state-action pairs Q-function from the samples of Q(s, a) that we observe during interaction with the enviornment. This approach is known as <strong class="gu ii">Time-Difference Learning.</strong></p><figure class="it iu iv iw ix iy eg eh paragraph-image"><div class="jk jl cr jm ai"><div class="eg eh kw"><div class="jd r cr id"><div class="kx r"><div class="od pz s t u ja ai av jb jc"><img class="s t u ja ai jf jg jh" src="./good explanation of RL_files/1_yoEaeSssoS7tx-8H24yQQQ.png" width="1364" height="210" role="presentation"></div><img class="cq iz s t u ja ai ji" width="1364" height="210" srcset="https://miro.medium.com/max/1104/1*yoEaeSssoS7tx-8H24yQQQ.png 552w, https://miro.medium.com/max/1400/1*yoEaeSssoS7tx-8H24yQQQ.png 700w" sizes="700px" role="presentation"><noscript><img class="s t u ja ai" src="https://miro.medium.com/max/2728/1*yoEaeSssoS7tx-8H24yQQQ.png" width="1364" height="210" srcSet="https://miro.medium.com/max/1104/1*yoEaeSssoS7tx-8H24yQQQ.png 552w, https://miro.medium.com/max/1400/1*yoEaeSssoS7tx-8H24yQQQ.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div><figcaption class="cc cp ko kp kq ei eg eh kr ks bx fx" data-selectable-paragraph="">Q-Learning algorithm</figcaption></figure><p id="0354" class="gs gt ap by gu b gv gw gx gy gz ha hb hc hd he hf eo" data-selectable-paragraph="">where ùõÇ is the learning rate. The <code class="id ie if ig ih b">Q(s,a)</code>table is initialized randomly. Then the agent starts to interact with the environment, and upon each interaction the agent will observe the reward of its action <code class="id ie if ig ih b">r(s,a)</code>and the state transition (new state <code class="id ie if ig ih b">s'</code>). The agent compute the observed Q-value Q_obs(s, a) and then use the above equation to update its own estimate of <code class="id ie if ig ih b">Q(s,a)</code> .</p><p id="b859" class="gs gt ap by gu b gv gw gx gy gz ha hb hc hd he hf eo" data-selectable-paragraph=""><strong class="gu ii">Exploration vs exploitation</strong></p><p id="3029" class="gs gt ap by gu b gv gw gx gy gz ha hb hc hd he hf eo" data-selectable-paragraph="">An important question is how does the agent select actions during learning. Should the agent trust the learnt values of Q(s, a) enough to select actions based on it ? or try other actions hoping this may give it a better reward. This is known as the exploration vs exploitation dilemma.</p><p id="432d" class="gs gt ap by gu b gv gw gx gy gz ha hb hc hd he hf eo" data-selectable-paragraph="">A simple approach is known as the ùõÜ-greedy approach where at each step. With small probability ùõú, the agent will pick a random action (explore) or with probability (1-ùõú) the agent will select an action according to the current estimate of Q-values. ùõú value can be decreased overtime as the agent becomes more confident with its estimate of Q-values.</p><h2 id="edf0" class="hk hl ap by bx hm hn ho hp hq hr hs ht hu hv hw hx" data-selectable-paragraph="">MountainCar Problem (using Q-Learning)</h2><figure class="it iu iv iw ix iy eg eh paragraph-image"><div class="eg eh ky"><div class="jd r cr id"><div class="kz r"><div class="od pz s t u ja ai av jb jc"><img class="s t u ja ai jf jg jh" src="./good explanation of RL_files/1_riQcuYHUPeg9adyv24kPHg.gif" width="600" height="400" role="presentation"></div><img class="cq iz s t u ja ai ji" width="600" height="400" srcset="https://miro.medium.com/max/1104/1*riQcuYHUPeg9adyv24kPHg.gif 552w, https://miro.medium.com/max/1200/1*riQcuYHUPeg9adyv24kPHg.gif 600w" sizes="600px" role="presentation"><noscript><img class="s t u ja ai" src="https://miro.medium.com/max/1200/1*riQcuYHUPeg9adyv24kPHg.gif" width="600" height="400" srcSet="https://miro.medium.com/max/1104/1*riQcuYHUPeg9adyv24kPHg.gif 552w, https://miro.medium.com/max/1200/1*riQcuYHUPeg9adyv24kPHg.gif 600w" sizes="600px" role="presentation"/></noscript></div></div></div><figcaption class="cc cp ko kp kq ei eg eh kr ks bx fx" data-selectable-paragraph="">Solution of MountainCar Problem using Q-Learning</figcaption></figure><p id="3bcc" class="gs gt ap by gu b gv gw gx gy gz ha hb hc hd he hf eo" data-selectable-paragraph="">Now, lets demonstrate how Q-Learning can be used to solve an interesting problem from OpenAI gym, the <a href="https://gym.openai.com/envs/MountainCar-v0" class="cu dw hg hh hi hj" target="_blank" rel="noopener nofollow">mountin-car</a> problem. In the mountain car problem, there is a car on 1-dimensional track between two mountains. The goal of the car is to climb the mountain on its right. However, its engine is not strong to climb the mountain without having to go back to gain some momentum by climbing the mountain on the left.<br>Here, the agent is the car, and possible actions are drive left, do nothing, or drive right. At every time step, the agent receives a penalty of <code class="id ie if ig ih b">-1</code> which means that the goal of the agent is to climb the right mountain as fast as possible to minimize the sum of <code class="id ie if ig ih b">-1</code> penalties it receives.</p><p id="e465" class="gs gt ap by gu b gv gw gx gy gz ha hb hc hd he hf eo" data-selectable-paragraph="">The observation is two continuous variables representing the velocity and position of the car. Since, the observation variables are continuous, for our algorithm we discretize the observed values in order to use Q-Learning. <br>While initially, the car is unable to climb the mountain and it will take forever, if you select a random action. After learning, it learns how to climb the mountain within less than <code class="id ie if ig ih b">100</code> time-steps.</p><figure class="it iu iv iw ix iy"><div class="jd r cr"><div class="qi r"><iframe src="./good explanation of RL_files/c0b047ca70a3c59b0d2b66ce83ca2c83.html" allowfullscreen="" frameborder="0" height="1882" width="680" title="Solution of MountainCar OpenAI Gym problem using Q-Learning." class="s t u ja ai" scrolling="auto"></iframe></div></div><figcaption class="cc cp ko kp kq ei eg eh kr ks bx fx">Solution of OpenAI Gym MountainCar Problem using Q-Learning.</figcaption></figure><h2 id="e22d" class="hk hl ap by bx hm hn ho hp hq hr hs ht hu hv hw hx" data-selectable-paragraph="">References</h2><ul class=""><li id="5656" class="gs gt ap by gu b gv hy gx hz gz ia hb ib hd ic hf ik il im" data-selectable-paragraph="">Alpaydin Introduction to Machine Learning, 3rd edition.</li><li id="ba7c" class="gs gt ap by gu b gv in gx io gz ip hb iq hd ir hf ik il im" data-selectable-paragraph=""><a href="http://web.stanford.edu/class/cs234/index.html" class="cu dw hg hh hi hj" target="_blank" rel="noopener nofollow">Stanford CS234 Reinforcement Learning</a></li><li id="fcfb" class="gs gt ap by gu b gv in gx io gz ip hb iq hd ir hf ik il im" data-selectable-paragraph=""><a href="http://ai.berkeley.edu/course_schedule.html" class="cu dw hg hh hi hj" target="_blank" rel="noopener nofollow">UC Berkley CS188 Introduction to AI</a></li></ul></div></div></section></div></article><div class="od en la lb ai li lg lj" data-test-id="post-sidebar"><div class="n p"><div class="z ab ac ae af ag ah ai"><div class="lk n ej"><div class="qk"><div class="ll lm ln n"><div class="n o"><div class="lo r cr"><div class=""><button class="bl lp lq lr ls lt lu lv gq lw lx"><svg width="29" height="29"><g fill-rule="evenodd"><path d="M13.74 1l.76 2.97.76-2.97zM16.82 4.78l1.84-2.56-1.43-.47zM10.38 2.22l1.84 2.56-.41-3.03zM22.38 22.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M9.1 22.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L6.1 15.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L6.4 11.26l-1.18-1.18a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L11.96 14a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L8.43 9.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L20.63 15c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM13 6.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 23 23.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div><div class="ly r"><div class="lz"><h4 class="bx fx cp bz cc"><button class="cu cv bg bh bi bj bk bl bm bn cw cx bq br cy cz">3.4K </button></h4></div></div></div></div><div class="lm r"></div><div><div class="gq"><div><div class="cl" role="tooltip" aria-hidden="true" aria-describedby="2" aria-labelledby="2"><button class="cu cv bg bh bi bj bk bl bm bn cw cx bq br cy cz"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></div><div class="od qk la lb lc ld le lf lg lh"></div><div><div class="ma iy n ej p"><div class="n p"><div class="z ab ac ae af et ah ai"><div class="n mb"></div><div class="n o mb"></div><div class="mc r"><ul class="bl bm"><li class="cl md gp me"><a href="https://medium.com/tag/machine-learning" class="mf mg dw cc r mh mi a b gf">Machine Learning</a></li><li class="cl md gp me"><a href="https://medium.com/tag/reinforcement-learning" class="mf mg dw cc r mh mi a b gf">Reinforcement Learning</a></li><li class="cl md gp me"><a href="https://medium.com/tag/deep-learning" class="mf mg dw cc r mh mi a b gf">Deep Learning</a></li><li class="cl md gp me"><a href="https://medium.com/tag/artificial-intelligence" class="mf mg dw cc r mh mi a b gf">Artificial Intelligence</a></li><li class="cl md gp me"><a href="https://medium.com/tag/openai" class="mf mg dw cc r mh mi a b gf">OpenAI</a></li></ul></div><div class="mj n fo y"><div class="n o"><div class="de r cr"><div class=""><div class="c mk dz n o ml cr mm mn mo mp mq mr ms mt mu mv mw mx my mz"><button class="bl lp lq lr ls lt na lv o ji dz n p nb u ja s t ai gq lw lx nc"><svg width="33" height="33" viewBox="0 0 33 33"><path d="M28.86 17.34l-3.64-6.4c-.3-.43-.71-.73-1.16-.8a1.12 1.12 0 0 0-.9.21c-.62.5-.73 1.18-.32 2.06l1.22 2.6 1.4 2.45c2.23 4.09 1.51 8-2.15 11.66a9.6 9.6 0 0 1-.8.71 6.53 6.53 0 0 0 4.3-2.1c3.82-3.82 3.57-7.87 2.05-10.39zm-6.25 11.08c3.35-3.35 4-6.78 1.98-10.47L21.2 12c-.3-.43-.71-.72-1.16-.8a1.12 1.12 0 0 0-.9.22c-.62.49-.74 1.18-.32 2.06l1.72 3.63a.5.5 0 0 1-.81.57l-8.91-8.9a1.33 1.33 0 0 0-1.89 1.88l5.3 5.3a.5.5 0 0 1-.71.7l-5.3-5.3-1.49-1.49c-.5-.5-1.38-.5-1.88 0a1.34 1.34 0 0 0 0 1.89l1.49 1.5 5.3 5.28a.5.5 0 0 1-.36.86.5.5 0 0 1-.36-.15l-5.29-5.29a1.34 1.34 0 0 0-1.88 0 1.34 1.34 0 0 0 0 1.89l2.23 2.23L9.3 21.4a.5.5 0 0 1-.36.85.5.5 0 0 1-.35-.14l-3.32-3.33a1.33 1.33 0 0 0-1.89 0 1.32 1.32 0 0 0-.39.95c0 .35.14.69.4.94l6.39 6.4c3.53 3.53 8.86 5.3 12.82 1.35zM12.73 9.26l5.68 5.68-.49-1.04c-.52-1.1-.43-2.13.22-2.89l-3.3-3.3a1.34 1.34 0 0 0-1.88 0 1.33 1.33 0 0 0-.4.94c0 .22.07.42.17.61zm14.79 19.18a7.46 7.46 0 0 1-6.41 2.31 7.92 7.92 0 0 1-3.67.9c-3.05 0-6.12-1.63-8.36-3.88l-6.4-6.4A2.31 2.31 0 0 1 2 19.72a2.33 2.33 0 0 1 1.92-2.3l-.87-.87a2.34 2.34 0 0 1 0-3.3 2.33 2.33 0 0 1 1.24-.64l-.14-.14a2.34 2.34 0 0 1 0-3.3 2.39 2.39 0 0 1 3.3 0l.14.14a2.33 2.33 0 0 1 3.95-1.24l.09.09c.09-.42.29-.83.62-1.16a2.34 2.34 0 0 1 3.3 0l3.38 3.39a2.17 2.17 0 0 1 1.27-.17c.54.08 1.03.35 1.45.76.1-.55.41-1.03.9-1.42a2.12 2.12 0 0 1 1.67-.4 2.8 2.8 0 0 1 1.85 1.25l3.65 6.43c1.7 2.83 2.03 7.37-2.2 11.6zM13.22.48l-1.92.89 2.37 2.83-.45-3.72zm8.48.88L19.78.5l-.44 3.7 2.36-2.84zM16.5 3.3L15.48 0h2.04L16.5 3.3z" fill-rule="evenodd"></path></svg></button></div></div></div><div class="ly r"><div class="lz"><h4 class="bx fx cp bz ap"><button class="cu cv bg bh bi bj bk bl bm bn cw cx bq br cy cz">3.4K claps</button></h4></div></div></div><div class="n o"><div class="go r bw"><a href="https://medium.com/p/978f9e89ddaa/share/twitter?source=post_actions_footer---------------------------" class="cu cv bg bh bi bj bk bl bm bn cw cx bq br cy cz" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="go r bw"><button class="cu cv bg bh bi bj bk bl bm bn cw cx bq br cy cz"><svg width="29" height="29" viewBox="0 0 29 29" fill="none" class="q"><path d="M5 6.36C5 5.61 5.63 5 6.4 5h16.2c.77 0 1.4.61 1.4 1.36v16.28c0 .75-.63 1.36-1.4 1.36H6.4c-.77 0-1.4-.6-1.4-1.36V6.36z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M10.76 20.9v-8.57H7.89v8.58h2.87zm-1.44-9.75c1 0 1.63-.65 1.63-1.48-.02-.84-.62-1.48-1.6-1.48-.99 0-1.63.64-1.63 1.48 0 .83.62 1.48 1.59 1.48h.01zM12.35 20.9h2.87v-4.79c0-.25.02-.5.1-.7.2-.5.67-1.04 1.46-1.04 1.04 0 1.46.8 1.46 1.95v4.59h2.87v-4.92c0-2.64-1.42-3.87-3.3-3.87-1.55 0-2.23.86-2.61 1.45h.02v-1.24h-2.87c.04.8 0 8.58 0 8.58z" fill="#fff"></path></svg></button></div><div class="go r bw"><a href="https://medium.com/p/978f9e89ddaa/share/facebook?source=post_actions_footer---------------------------" class="cu cv bg bh bi bj bk bl bm bn cw cx bq br cy cz" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="nd r bw"><div><div class="gq"><div><div class="cl" role="tooltip" aria-hidden="true" aria-describedby="3" aria-labelledby="3"><button class="cu cv bg bh bi bj bk bl bm bn cw cx bq br cy cz"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div><div class="cl" aria-hidden="true"><div class="r bw"><button class="cu cv bg bh bi bj bk bl bm bn cw cx bq br cy cz"><svg width="25" height="25" viewBox="-480.5 272.5 21 21" class="q"><path d="M-463 284.6c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5z"></path></svg></button></div></div></div></div><div class="ne nf ng mc r y"><div class="nh ni r cr"><span class="r nj al nk"><div class="r s nl nm"><a rel="noopener" href="https://medium.com/@m.alzantot?source=follow_footer--------------------------follow_footer-"><img alt="Moustafa Alzantot" class="r dz nn no" src="./good explanation of RL_files/0_vbsTKQhLiZPMFy9f_(1)" width="80" height="80"></a></div><span class="r"><div class="np r nq"><p class="bx fx gf bz cc nr ns">Written by</p></div><div class="np nt n nq"><div class="ai n o fo"><h2 class="bx hm nu nv ap"><a class="cu cv bg bh bi bj bk bl bm bn cw cx bq br cy cz" rel="noopener" href="https://medium.com/@m.alzantot?source=follow_footer--------------------------follow_footer-">Moustafa Alzantot</a></h2><div class="r g"><button class="dg dh be bf gc bo bp gd bn ds bx b by bz ca cb dt du dv cl dw bq">Follow</button></div></div></div></span></span><div class="np nw r nq bb"><div class="nx r"><h4 class="bx fx ny nz cc">Computer Science PhD Student at UCLA. (http://web.cs.ucla.edu/~malzantot/)</h4></div><div class="ba oa bb"><button class="dg dh be bf gc bo bp gd bn ds bx b by bz ca cb dt du dv cl dw bq">Follow</button></div></div></div></div><div class="ob nf r y"><a href="https://medium.com/p/978f9e89ddaa/responses/show?source=follow_footer--------------------------follow_footer-" class="cu cv bg bh bi bj bk bl bm bn cw cx bq br cy cz" rel="noopener"><span class="oc od ls"><div class="oe of ds r kq bb"><span class="be">See responses (32)</span></div></span></a></div></div></div><div class="og r oh y"><div class="n p"><div class="z ab ac ae af ag ah ai"></div></div></div></div></div><div class="oi r oj ok"><section class="eg eh ai dv r ol om on oo op oq or os ot ou ov ow ox oy oz"><div class="pa pb nh n fo g"><div class="pc n fo"><div class="pd r pe"><div class="pf r"><a href="https://medium.com/about?autoplay=1&amp;source=post_page-----978f9e89ddaa----------------------" class="cu cv bg bh bi bj bk bl bm bn pg ph bq br pi pj" rel="noopener"><h4 class="pk pl pm bx hm by nz pn po r">Discover <!-- -->Medium</h4></a></div><span class="bx b by bz ca cb r pp pq">Welcome to a place where words matter. On <!-- -->Medium<!-- -->, smart voices and original ideas take center stage - with no ads in sight.<!-- --> <a href="https://medium.com/about?autoplay=1&amp;source=post_page-----978f9e89ddaa----------------------" class="cu cv bg bh bi bj bk bl bm bn bq br pi pj pr" rel="noopener">Watch</a></span></div><div class="pd r pe"><div class="ps r"><a href="https://medium.com/topics?source=post_page-----978f9e89ddaa----------------------" class="cu cv bg bh bi bj bk bl bm bn pg ph bq br pi pj" rel="noopener"><h4 class="pk pl pm bx hm by nz pn po r">Make <!-- -->Medium<!-- --> yours</h4></a></div><span class="bx b by bz ca cb r pp pq">Follow all the topics you care about, and we‚Äôll deliver the best stories for you to your homepage and inbox.<!-- --> <a href="https://medium.com/topics?source=post_page-----978f9e89ddaa----------------------" class="cu cv bg bh bi bj bk bl bm bn bq br pi pj pr" rel="noopener">Explore</a></span></div><div class="pd r pe"><div class="pf r"><a class="cu cv bg bh bi bj bk bl bm bn pg ph bq br pi pj" rel="noopener" href="https://medium.com/membership?source=post_page-----978f9e89ddaa----------------------"><h4 class="pk pl pm bx hm by nz pn po r">Become a member</h4></a></div><span class="bx b by bz ca cb r pp pq">Get unlimited access to the best stories on <!-- -->Medium<!-- --> ‚Äî and support writers while you‚Äôre at it. Just $5/month.<!-- --> <a class="cu cv bg bh bi bj bk bl bm bn bq br pi pj pr" rel="noopener" href="https://medium.com/membership?source=post_page-----978f9e89ddaa----------------------">Upgrade</a></span></div></div></div><div class="n o fo"><a href="https://medium.com/?source=post_page-----978f9e89ddaa----------------------" class="cu cv bg bh bi bj bk bl bm bn pg ph bq br pi pj" rel="noopener"><svg height="22" width="112" viewBox="0 0 111.5 22" class="pl"><path d="M56.3 19.5c0 .4 0 .5.3.7l1.5 1.4v.1h-6.5V19c-.7 1.8-2.4 3-4.3 3-3.3 0-5.8-2.6-5.8-7.5 0-4.5 2.6-7.6 6.3-7.6 1.6-.1 3.1.8 3.8 2.4V3.2c0-.3-.1-.6-.3-.7l-1.4-1.4V1l6.5-.8v19.3zm-4.8-.8V9.5c-.5-.6-1.2-.9-1.9-.9-1.6 0-3.1 1.4-3.1 5.7 0 4 1.3 5.4 3 5.4.8.1 1.6-.3 2-1zm9.1 3.1V9.4c0-.3-.1-.6-.3-.7l-1.4-1.5v-.1h6.5v12.5c0 .4 0 .5.3.7l1.4 1.4v.1h-6.5zm-.2-19.2C60.4 1.2 61.5 0 63 0c1.4 0 2.6 1.2 2.6 2.6S64.4 5.3 63 5.3a2.6 2.6 0 0 1-2.6-2.7zm22.5 16.9c0 .4 0 .5.3.7l1.5 1.4v.1h-6.5v-3.2c-.6 2-2.4 3.4-4.5 3.4-2.9 0-4.4-2.1-4.4-6.2 0-1.9 0-4.1.1-6.5 0-.3-.1-.5-.3-.7L67.7 7v.1H74v8c0 2.6.4 4.4 2 4.4.9-.1 1.7-.6 2.1-1.3V9.5c0-.3-.1-.6-.3-.7l-1.4-1.5v-.2h6.5v12.4zm22 2.3c0-.5.1-6.5.1-7.9 0-2.6-.4-4.5-2.2-4.5-.9 0-1.8.5-2.3 1.3.2.8.3 1.7.3 2.5 0 1.8-.1 4.2-.1 6.5 0 .3.1.5.3.7l1.5 1.4v.1H96c0-.4.1-6.5.1-7.9 0-2.7-.4-4.5-2.2-4.5-.9 0-1.7.5-2.2 1.3v9c0 .4 0 .5.3.7l1.4 1.4v.1h-6.5V9.5c0-.3-.1-.6-.3-.7l-1.4-1.5v-.2h6.5v3.1a4.6 4.6 0 0 1 4.6-3.4c2.2 0 3.6 1.2 4.2 3.5.7-2.1 2.7-3.6 4.9-3.5 2.9 0 4.5 2.2 4.5 6.2 0 1.9-.1 4.2-.1 6.5-.1.3.1.6.3.7l1.4 1.4v.1h-6.6zm-81.4-2l1.9 1.9v.1h-9.8v-.1l2-1.9c.2-.2.3-.4.3-.7V7.3c0-.5 0-1.2.1-1.8L11.4 22h-.1L4.5 6.8c-.1-.4-.2-.4-.3-.6v10c-.1.7 0 1.3.3 1.9l2.7 3.6v.1H0v-.1L2.7 18c.3-.6.4-1.3.3-1.9v-11c0-.5-.1-1.1-.5-1.5L.7 1.1V1h7l5.8 12.9L18.6 1h6.8v.1l-1.9 2.2c-.2.2-.3.5-.3.7v15.2c0 .2.1.5.3.6zm7.6-5.9c0 3.8 1.9 5.3 4.2 5.3 1.9.1 3.6-1 4.4-2.7h.1c-.8 3.7-3.1 5.5-6.5 5.5-3.7 0-7.2-2.2-7.2-7.4 0-5.5 3.5-7.6 7.3-7.6 3.1 0 6.4 1.5 6.4 6.2v.8h-8.7zm0-.8h4.3v-.8c0-3.9-.8-4.9-2-4.9-1.4.1-2.3 1.6-2.3 5.7z"></path></svg></a><span class="bx b by bz ca cb r pp pq"><div class="pt pu n fo pv al"><a href="https://medium.com/about?autoplay=1&amp;source=post_page-----978f9e89ddaa----------------------" class="cu cv bg bh bi bj bk bl bm bn fz bq br pi pj" rel="noopener">About</a><a href="https://help.medium.com/?source=post_page-----978f9e89ddaa----------------------" class="cu cv bg bh bi bj bk bl bm bn fz bq br pi pj" rel="noopener">Help</a><a class="cu cv bg bh bi bj bk bl bm bn fz bq br pi pj" rel="noopener" href="https://medium.com/policy/9db0094a1e0f?source=post_page-----978f9e89ddaa----------------------">Legal</a></div></span></div></section></div></div></div><script src="./good explanation of RL_files/16180790160.js.download"></script><iframe src="./good explanation of RL_files/a16180790160.html" hidden="" tabindex="-1" title="Optimizely Internal Frame" height="0" width="0" style="display: none;"></iframe><script>window.__BUILD_ID__ = "master-20200330-223244-87639e84cb"</script><script>window.__GRAPHQL_URI__ = "https://medium.com/_/graphql"</script><script>window.__PRELOADED_STATE__ = {"config":{"nodeEnv":"production","version":"master-20200330-223244-87639e84cb","productName":"Medium","publicUrl":"https:\u002F\u002Fcdn-client.medium.com\u002Flite","authDomain":"medium.com","authGoogleClientId":"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com","favicon":"production","glyphUrl":"https:\u002F\u002Fglyph.medium.com","branchKey":"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm","lightStep":{"name":"lite-web","host":"collector-medium.lightstep.com","token":"ce5be895bef60919541332990ac9fef2","appVersion":"master-20200330-223244-87639e84cb"},"algolia":{"appId":"MQ57UUUQZ2","apiKeySearch":"394474ced050e3911ae2249ecc774921","indexPrefix":"medium_","host":"-dsn.algolia.net"},"recaptchaKey":"6Lfc37IUAAAAAKGGtC6rLS13R1Hrw_BqADfS1LRk","recaptcha3Key":"6Lf8R9wUAAAAABMI_85Wb8melS7Zj6ziuf99Yot5","datadog":{"clientToken":"pub853ea8d17ad6821d9f8f11861d23dfed","context":{"deployment":{"target":"production","tag":"master-20200330-223244-87639e84cb","commit":"87639e84cb194c04e206a1f8f615214766a80256"}},"datacenter":"us"},"sentry":{"dsn":"https:\u002F\u002F589e367c28ca47b195ce200d1507d18b@sentry.io\u002F1423575","environment":"production"},"isAmp":false,"googleAnalyticsCode":"UA-24232453-2","signInWallCustomDomainCollectionIds":["3a8144eabfe3","336d898217ee","61061eb0c96b","138adf9c44c","819cc2aaeee0"],"mediumOwnedAndOperatedCollectionIds":["544c7006046e","bcc38c8f6edf","444d13b52878","8d6b8a439e32","92d2092dc598","1285ba81cada","cb8577c9149e","8ccfed20cbb2","ae2a65f35510","3f6ecf56618","7b6769f2748b","fc8964313712","ef8e90590e66","191186aaafa0","d944778ce714","bdc4052bbdba","88d9857e584e"],"tierOneDomains":["medium.com","thebolditalic.com","arcdigital.media","towardsdatascience.com","uxdesign.cc","codeburst.io","psiloveyou.xyz","writingcooperative.com","entrepreneurshandbook.co","prototypr.io","betterhumans.coach.me","theascent.pub"],"internalLinksPostIds":["0000","0001","0002","0003"],"defaultImages":{"avatar":{"imageId":"1*dmbNkD5D-u45r44go_cf0g.png","height":150,"width":150},"orgLogo":{"imageId":"1*OMF3fSqH8t4xBJ9-6oZDZw.png","height":106,"width":545},"postLogo":{"imageId":"1*3sela1OADrJr7dJk_CXaEQ.png","height":810,"width":1440}},"performanceTags":[],"collectionStructuredData":{"8d6b8a439e32":{"name":"Elemental","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F980\u002F1*9ygdqoKprhwuTVKUM0DLPA@2x.png","width":980,"height":159}}},"3f6ecf56618":{"name":"Forge","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F596\u002F1*uULpIlImcO5TDuBZ6lm7Lg@2x.png","width":596,"height":183}}},"ae2a65f35510":{"name":"GEN","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F264\u002F1*RdVZMdvfV3YiZTw6mX7yWA.png","width":264,"height":140}}},"88d9857e584e":{"name":"LEVEL","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*JqYMhNX6KNNb2UlqGqO2WQ.png","width":540,"height":108}}},"7b6769f2748b":{"name":"Marker","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F383\u002F1*haCUs0wF6TgOOvfoY-jEoQ@2x.png","width":383,"height":92}}},"444d13b52878":{"name":"OneZero","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*cw32fIqCbRWzwJaoQw6BUg.png","width":540,"height":123}}},"8ccfed20cbb2":{"name":"Zora","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*tZUQqRcCCZDXjjiZ4bDvgQ.png","width":540,"height":106}}}},"embeddedPostIds":{"coronavirus":"cd3010f9d81f"},"covidCollectionId":"8a9336e5bb4","sharedCdcMessaging":{"COVID_APPLICABLE_TAG_SLUGS":["pandemic","epidemic","coronavirus","covid19","covid-19","co-vid-19","containment","self-care","flatten-the-curve","public-health","virus","public-health-crisis","quarantine","self-quarantine","zika","corona","disease-prevention","wuhan","chinavirus","outbreak","influenza","socialdistancing","social-distance","flu","vaccines","healthcare","medicine","conspiracy-theories","conspiracy","virality","epidemia","pandemia","salud","corona-e-virus","coronavirus-covid19"],"COVID_APPLICABLE_TOPIC_NAMES":["coronavirus","health"],"COVID_MESSAGES":{"tierA":{"text":"For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":66,"end":73,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"tierB":{"text":"Anyone can publish on Medium per our Policies, but we don‚Äôt fact-check every story. For more info about the coronavirus, see cdc.gov.","markups":[{"start":37,"end":45,"href":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Fcategories\u002F201931128-Policies-Safety"},{"start":125,"end":132,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"paywall":{"text":"This article has been made free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":56,"end":70,"href":"https:\u002F\u002Fmedium.com\u002Fmembership"},{"start":138,"end":145,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]}},"COVID_BANNER_POST_ID_OVERRIDE_WHITELIST":["3b31a67bff4a"]},"embedPostRules":[]},"debug":{"requestId":"4144e899-b179-4ac5-9e2e-74048ef8b936","originalSpanCarrier":{"ot-tracer-spanid":"1f7a8dd455b55de2","ot-tracer-traceid":"7842a355558a2622","ot-tracer-sampled":"true"}},"session":{"user":{"id":"d123089bb9ce"},"xsrf":"aROWLyD7bzzh"},"stats":{"itemCount":0,"sending":false,"timeout":null,"backup":{}},"navigation":{"branch":{"show":null,"hasRendered":null,"blockedByCTA":true},"hideGoogleOneTap":false,"hasRenderedGoogleOneTap":null,"currentLocation":"https:\u002F\u002Fmedium.com\u002F@m.alzantot\u002Fdeep-reinforcement-learning-demysitifed-episode-2-policy-iteration-value-iteration-and-q-978f9e89ddaa","host":"medium.com","hostname":"medium.com","referrer":"https:\u002F\u002Fwww.google.com\u002F","susiModal":{"step":null,"operation":"register","reportEventInfo":{"eventName":"","data":{}}},"postRead":false},"client":{"isBot":false,"isEu":false,"isLinkedin":false,"isNativeMedium":false,"isCustomDomain":false},"multiVote":{"clapsPerPost":{}}}</script><script>window.__APOLLO_STATE__ = {"ROOT_QUERY.variantFlags.0":{"name":"add_friction_to_signup","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.0.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.0.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.1":{"name":"allow_access","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.1.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.1.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.2":{"name":"allow_signup","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.2.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.2.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.3":{"name":"allow_test_auth","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.3.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.3.valueType":{"__typename":"VariantFlagString","value":"disallow"},"ROOT_QUERY.variantFlags.4":{"name":"assign_default_topic_to_posts","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.4.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.4.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.5":{"name":"available_annual_plan","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.5.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.5.valueType":{"__typename":"VariantFlagString","value":"2c754bcc2995"},"ROOT_QUERY.variantFlags.6":{"name":"available_monthly_plan","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.6.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.6.valueType":{"__typename":"VariantFlagString","value":"60e220181034"},"ROOT_QUERY.variantFlags.7":{"name":"branch_seo_metadata","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.7.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.7.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.8":{"name":"browsable_stream_config_bucket","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.8.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.8.valueType":{"__typename":"VariantFlagString","value":"curated-topics"},"ROOT_QUERY.variantFlags.9":{"name":"coronavirus_topic_recirc","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.9.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.9.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.10":{"name":"covid_19_cdc_banner","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.10.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.10.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.11":{"name":"disable_android_subscription_activity_carousel","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.11.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.11.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.12":{"name":"disable_gosocial_followers_that_you_follow","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.12.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.12.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.13":{"name":"disable_ios_resume_reading_toast","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.13.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.13.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.14":{"name":"disable_ios_subscription_activity_carousel","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.14.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.14.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.15":{"name":"disable_mobile_featured_chunk","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.15.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.15.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.16":{"name":"disable_post_recommended_from_friends_provider","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.16.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.16.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.17":{"name":"enable_android_local_currency","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.17.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.17.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.18":{"name":"enable_annual_renewal_reminder_email","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.18.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.18.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.19":{"name":"enable_app_flirty_thirty","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.19.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.19.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.20":{"name":"enable_apple_sign_in_ios","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.20.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.20.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.21":{"name":"enable_auto_tier","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.21.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.21.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.22":{"name":"enable_automated_mission_control_triggers","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.22.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.22.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.23":{"name":"enable_branch_io","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.23.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.23.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.24":{"name":"enable_branding","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.24.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.24.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.25":{"name":"enable_branding_fonts","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.25.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.25.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.26":{"name":"enable_curation_priority_queue_experiment","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.26.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.26.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.27":{"name":"enable_dedicated_series_tab_api_ios","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.27.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.27.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.28":{"name":"enable_different_grid","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.28.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.28.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.29":{"name":"enable_digest_feature_logging","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.29.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.29.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.30":{"name":"enable_disregard_trunc_state_for_footer","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.30.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.30.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.31":{"name":"enable_edit_alt_text","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.31.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.31.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.32":{"name":"enable_email_sign_in_captcha","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.32.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.32.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.33":{"name":"enable_embedding_based_diversification","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.33.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.33.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.34":{"name":"enable_ev_mission_email_v3","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.34.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.34.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.35":{"name":"enable_expanded_feature_chunk_pool","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.35.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.35.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.36":{"name":"enable_filter_by_resend_rules","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.36.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.36.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.37":{"name":"enable_filter_expire_processor","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.37.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.37.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.38":{"name":"enable_first_name_on_paywall","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.38.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.38.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.39":{"name":"enable_free_corona_topic","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.39.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.39.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.40":{"name":"enable_global_susi_modal","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.40.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.40.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.41":{"name":"enable_google_one_tap","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.41.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.41.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.42":{"name":"enable_ios_post_stats","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.42.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.42.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.43":{"name":"enable_janky_spam_rules","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.43.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.43.valueType":{"__typename":"VariantFlagString","value":"users,posts"},"ROOT_QUERY.variantFlags.44":{"name":"enable_json_logs_trained_ranker","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.44.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.44.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.45":{"name":"enable_kafka_events","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.45.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.45.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.46":{"name":"enable_kbfd_rex","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.46.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.46.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.47":{"name":"enable_kbfd_rex_app_highlights","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.47.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.47.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.48":{"name":"enable_kbfd_rex_daily_digest","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.48.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.48.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.49":{"name":"enable_li_open_in_app","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.49.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.49.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.50":{"name":"enable_lite_notifications","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.50.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.50.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.51":{"name":"enable_lite_post","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.51.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.51.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.52":{"name":"enable_lite_post_cd","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.52.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.52.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.53":{"name":"enable_lite_post_highlights","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.53.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.53.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.54":{"name":"enable_lite_post_highlights_view_only","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.54.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.54.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.55":{"name":"enable_lite_profile","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.55.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.55.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.56":{"name":"enable_lite_pub_header_menu","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.56.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.56.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.57":{"name":"enable_lite_server_upstream_deadlines","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.57.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.57.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.58":{"name":"enable_lite_stories","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.58.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.58.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.59":{"name":"enable_lite_topics","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.59.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.59.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.60":{"name":"enable_lite_unread_notification_count_mutation","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.60.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.60.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.61":{"name":"enable_lo_meter_swap","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.61.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.61.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.62":{"name":"enable_lo_open_in_app","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.62.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.62.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.63":{"name":"enable_logged_out_homepage_signup","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.63.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.63.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.64":{"name":"enable_login_code_flow","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.64.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.64.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.65":{"name":"enable_marketing_emails","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.65.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.65.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.66":{"name":"enable_media_resource_try_catch","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.66.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.66.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.67":{"name":"enable_membership_remove_section_a","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.67.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.67.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.68":{"name":"enable_minimal_meter_v2","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.68.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.68.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.69":{"name":"enable_miro_on_kubernetes","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.69.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.69.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.70":{"name":"enable_mk_branch_cleanup","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.70.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.70.valueType":{"__typename":"VariantFlagString","value":"app-button"},"ROOT_QUERY.variantFlags.71":{"name":"enable_ml_rank_modules","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.71.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.71.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.72":{"name":"enable_monthly_membership_default","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.72.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.72.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.73":{"name":"enable_more_on_coronavirus","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.73.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.73.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.74":{"name":"enable_mute","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.74.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.74.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.75":{"name":"enable_new_collaborative_filtering_data","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.75.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.75.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.76":{"name":"enable_new_suspended_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.76.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.76.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.77":{"name":"enable_new_three_dot_menu","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.77.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.77.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.78":{"name":"enable_new_training_data_pipeline_mmr_group_a","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.78.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.78.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.79":{"name":"enable_newsletter_v3_landing_for_covid_only","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.79.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.79.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.80":{"name":"enable_optimizely","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.80.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.80.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.81":{"name":"enable_pardon_the_interruption_removal","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.81.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.81.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.82":{"name":"enable_parsely","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.82.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.82.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.83":{"name":"enable_patronus_on_kubernetes","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.83.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.83.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.84":{"name":"enable_popularity_feature","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.84.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.84.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.85":{"name":"enable_post_import","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.85.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.85.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.86":{"name":"enable_post_page_nav_stickiness_removal","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.86.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.86.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.87":{"name":"enable_post_seo_settings_screen","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.87.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.87.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.88":{"name":"enable_post_settings_screen","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.88.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.88.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.89":{"name":"enable_primary_topic_for_mobile","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.89.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.89.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.90":{"name":"enable_represent_ml","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.90.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.90.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.91":{"name":"enable_rito_upstream_deadlines","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.91.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.91.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.92":{"name":"enable_rtr_channel","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.92.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.92.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.93":{"name":"enable_save_to_medium","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.93.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.93.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.94":{"name":"enable_starspace","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.94.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.94.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.95":{"name":"enable_suggest_account","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.95.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.95.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.96":{"name":"enable_suggest_account_li","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.96.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.96.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.97":{"name":"enable_tick_landing_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.97.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.97.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.98":{"name":"enable_tipalti_onboarding","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.98.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.98.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.99":{"name":"enable_topic_lifecycle_email","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.99.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.99.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.100":{"name":"enable_tribute_landing_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.100.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.100.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.101":{"name":"enable_trumpland_landing_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.101.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.101.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.102":{"name":"enable_utc_fix_on_partner_program_dashboard","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.102.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.102.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.103":{"name":"featured_fc_and_ydr","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.103.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.103.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.104":{"name":"filter_low_scoring_users","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.104.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.104.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.105":{"name":"glyph_embed_commands","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.105.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.105.valueType":{"__typename":"VariantFlagString","value":"none"},"ROOT_QUERY.variantFlags.106":{"name":"glyph_font_set","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.106.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.106.valueType":{"__typename":"VariantFlagString","value":"m2"},"ROOT_QUERY.variantFlags.107":{"name":"google_sign_in_android","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.107.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.107.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.108":{"name":"is_not_medium_subscriber","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.108.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.108.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.109":{"name":"make_nav_sticky","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.109.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.109.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.110":{"name":"new_transition_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.110.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.110.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.111":{"name":"pardon_the_interruption_4","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.111.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.111.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.112":{"name":"pub_sidebar","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.112.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.112.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.113":{"name":"rank_model","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.113.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.113.valueType":{"__typename":"VariantFlagString","value":"default"},"ROOT_QUERY.variantFlags.114":{"name":"redis_read_write_splitting","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.114.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.114.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.115":{"name":"share_post_linkedin","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.115.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.115.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.116":{"name":"sign_up_with_email_button","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.116.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.116.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.117":{"name":"signin_services","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.117.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.117.valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap,apple"},"ROOT_QUERY.variantFlags.118":{"name":"signup_services","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.118.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.118.valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap,apple"},"ROOT_QUERY.variantFlags.119":{"name":"use_new_admin_topic_backend","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.119.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.119.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY":{"variantFlags":[{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.0","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.1","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.2","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.3","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.4","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.5","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.6","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.7","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.8","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.9","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.10","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.11","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.12","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.13","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.14","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.15","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.16","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.17","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.18","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.19","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.20","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.21","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.22","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.23","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.24","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.25","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.26","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.27","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.28","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.29","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.30","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.31","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.32","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.33","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.34","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.35","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.36","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.37","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.38","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.39","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.40","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.41","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.42","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.43","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.44","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.45","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.46","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.47","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.48","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.49","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.50","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.51","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.52","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.53","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.54","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.55","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.56","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.57","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.58","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.59","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.60","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.61","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.62","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.63","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.64","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.65","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.66","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.67","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.68","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.69","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.70","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.71","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.72","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.73","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.74","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.75","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.76","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.77","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.78","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.79","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.80","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.81","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.82","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.83","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.84","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.85","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.86","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.87","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.88","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.89","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.90","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.91","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.92","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.93","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.94","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.95","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.96","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.97","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.98","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.99","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.100","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.101","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.102","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.103","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.104","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.105","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.106","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.107","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.108","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.109","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.110","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.111","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.112","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.113","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.114","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.115","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.116","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.117","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.118","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.119","typename":"VariantFlag"}],"viewer":{"type":"id","generated":false,"id":"User:d123089bb9ce","typename":"User"},"meterPost({\"postId\":\"978f9e89ddaa\",\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}})":{"type":"id","generated":false,"id":"MeteringInfo:singleton","typename":"MeteringInfo"},"postResult({\"id\":\"978f9e89ddaa\"})":{"type":"id","generated":false,"id":"Post:978f9e89ddaa","typename":"Post"}},"User:d123089bb9ce":{"id":"d123089bb9ce","username":"rixazeez","name":"Rixazeez","imageId":"0*Yf7A5FNFwvWBahQb.jpg","mediumMemberAt":0,"hasPastMemberships":false,"isPartnerProgramEnrolled":false,"email":"rixazeez@gmail.com","unverifiedEmail":"","createdAt":1582506277846,"__typename":"User"},"MeteringInfo:singleton":{"__typename":"MeteringInfo","postIds":{"type":"json","json":[]},"maxUnlockCount":4,"unlocksRemaining":4},"Post:978f9e89ddaa":{"__typename":"Post","id":"978f9e89ddaa","visibility":"PUBLIC","latestPublishedVersion":"2e6d156bec95","collection":null,"creator":{"type":"id","generated":false,"id":"User:a504f9e1d3cd","typename":"User"},"isLocked":false,"lockedSource":"LOCKED_POST_SOURCE_NONE","sequence":null,"mediumUrl":"https:\u002F\u002Fmedium.com\u002F@m.alzantot\u002Fdeep-reinforcement-learning-demysitifed-episode-2-policy-iteration-value-iteration-and-q-978f9e89ddaa","canonicalUrl":"","content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}})":{"type":"id","generated":true,"id":"$Post:978f9e89ddaa.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}})","typename":"PostContent"},"firstPublishedAt":1499574345619,"isPublished":true,"layerCake":0,"primaryTopic":null,"title":"Deep Reinforcement Learning Demysitifed (Episode 2) ‚Äî Policy Iteration, Value Iteration and‚Ä¶","isLimitedState":false,"pendingCollection":null,"shareKey":null,"statusForCollection":null,"readingTime":10.724528301886792,"readingList":"READING_LIST_NONE","allowResponses":true,"clapCount":3490,"viewerClapCount":null,"license":"ALL_RIGHTS_RESERVED","tags":[{"type":"id","generated":false,"id":"Tag:machine-learning","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:reinforcement-learning","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:deep-learning","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:artificial-intelligence","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:openai","typename":"Tag"}],"topics":[],"voterCount":583,"recommenders":[],"postResponses":{"type":"id","generated":true,"id":"$Post:978f9e89ddaa.postResponses","typename":"PostResponses"},"responsesCount":32,"collaborators":[],"translationSourcePost":null,"newsletterId":"","inResponseToPostResult":null,"inResponseToMediaResource":null,"curationEligibleAt":0,"isDistributionAlertDismissed":false,"audioVersionUrl":"","seoTitle":"","socialTitle":"","socialDek":"","metaDescription":"","latestPublishedAt":1538962888489,"previewContent":{"type":"id","generated":true,"id":"$Post:978f9e89ddaa.previewContent","typename":"PreviewContent"},"previewImage":{"type":"id","generated":false,"id":"ImageMetadata:1*riQcuYHUPeg9adyv24kPHg.gif","typename":"ImageMetadata"},"updatedAt":1538962888489,"seoDescription":"","isSuspended":false},"User:a504f9e1d3cd":{"id":"a504f9e1d3cd","__typename":"User","isSuspended":false,"allowNotes":true,"name":"Moustafa Alzantot","isFollowing":false,"username":"m.alzantot","bio":"Computer Science PhD Student at UCLA. (http:\u002F\u002Fweb.cs.ucla.edu\u002F~malzantot\u002F)","imageId":"0*vbsTKQhLiZPMFy9f.","mediumMemberAt":0,"isBlocking":false,"isMuting":false,"isPartnerProgramEnrolled":false,"twitterScreenName":"m_alzantot"},"$Post:978f9e89ddaa.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}})":{"isLockedPreviewOnly":false,"validatedShareKey":"","__typename":"PostContent","bodyModel":{"type":"id","generated":true,"id":"$Post:978f9e89ddaa.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel","typename":"RichText"}},"$Post:978f9e89ddaa.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.sections.0":{"name":"f9cb","startIndex":0,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null,"__typename":"Section"},"$Post:978f9e89ddaa.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel":{"sections":[{"type":"id","generated":true,"id":"$Post:978f9e89ddaa.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.sections.0","typename":"Section"}],"paragraphs":[{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_0","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_1","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_2","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_3","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_4","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_5","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_6","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_7","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_8","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_9","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_10","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_11","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_12","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_13","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_14","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_15","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_16","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_17","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_18","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_19","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_20","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_21","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_22","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_23","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_24","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_25","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_26","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_27","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_28","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_29","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_30","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_31","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_32","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_33","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_34","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_35","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_36","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_37","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_38","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_39","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_40","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_41","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_42","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_43","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_44","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_45","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_46","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_47","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_48","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_49","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_50","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_51","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_52","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_53","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_54","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_55","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_56","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_57","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_58","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_59","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_60","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_61","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_62","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_63","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_64","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_65","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_66","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_67","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_68","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_69","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_70","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_71","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_72","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_73","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_74","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_75","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_76","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_77","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_78","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_79","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_80","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_81","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_82","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:2e6d156bec95_83","typename":"Paragraph"}],"__typename":"RichText"},"Paragraph:2e6d156bec95_0":{"id":"2e6d156bec95_0","name":"6b57","type":"H3","href":null,"layout":null,"metadata":null,"text":"Deep Reinforcement Learning Demysitifed (Episode 2) ‚Äî Policy Iteration, Value Iteration and Q-learning","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_0.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_0.markups.0":{"type":"STRONG","start":0,"end":102,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_1":{"id":"2e6d156bec95_1","name":"4179","type":"P","href":null,"layout":null,"metadata":null,"text":"In previous two articles, we introduced reinforcement learning definition, examples, and simple solving strategies using random policy search and genetic algorithms.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_1.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_1.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_1.markups.0":{"type":"A","start":121,"end":141,"href":"https:\u002F\u002Fmedium.com\u002F@m.alzantot\u002Fdeep-reinforcement-learning-demystified-episode-0-2198c05a6124","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_1.markups.1":{"type":"A","start":146,"end":164,"href":"https:\u002F\u002Fbecominghuman.ai\u002Fgenetic-algorithm-for-reinforcement-learning-a38a5612c4dc","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_2":{"id":"2e6d156bec95_2","name":"9db7","type":"P","href":null,"layout":null,"metadata":null,"text":"In practice, random search does not work well for complex problems where the search space (that depends on the number of possible states and actions) is large. Also, genetic algorithm is a meta-heuristic optimization so it does not provide a guarantee to find an optimal solution. In this article, we are going to introduce fundamental reinforcement learning algorithms.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_3":{"id":"2e6d156bec95_3","name":"4528","type":"P","href":null,"layout":null,"metadata":null,"text":"We start by reviewing the Markov Decision Process formulation, then we describe the value-iteration and policy iteration which are algorithms for finding the optimal policy when the agent knows sufficient details about the environment model. We then, describe the Q-learning is a model-free learning that can be used when the agent does not know the environment model but has to discover the policy by trial and error making use of its history of interaction with the environment. We also provide demonstration examples of the three methods by using the FrozenLake8x8 and MountainCar problems from OpenAI gym.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_3.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_3.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_3.markups.0":{"type":"A","start":554,"end":567,"href":"https:\u002F\u002Fgym.openai.com\u002Fenvs\u002FFrozenLake8x8-v0","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_3.markups.1":{"type":"A","start":572,"end":583,"href":"https:\u002F\u002Fgym.openai.com\u002Fenvs\u002FMountainCar-v0","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_4":{"id":"2e6d156bec95_4","name":"9611","type":"H4","href":null,"layout":null,"metadata":null,"text":"Markov Decision Process (MDP)","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_5":{"id":"2e6d156bec95_5","name":"692e","type":"P","href":null,"layout":null,"metadata":null,"text":"We briefly introduced Markov Decision Process MDPin our first article. To recall, in reinforcement learning problems we have an agent interacting with an environment. At each time step, the agent performs an action which leads to two things: changing the environment state and the agent (possibly) receiving a reward (or penalty) from the environment. The goal of the agent is to discover an optimal policy (i.e. what actions to do in each state) such that it maximizes the total value of rewards received from the environment in response to its actions. MDPis used to describe the agent\u002F environment interaction settings in a formal way.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_5.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_5.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_5.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_5.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_5.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_5.markups.5","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_5.markups.0":{"type":"CODE","start":46,"end":49,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_5.markups.1":{"type":"CODE","start":555,"end":558,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_5.markups.2":{"type":"STRONG","start":46,"end":49,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_5.markups.3":{"type":"STRONG","start":555,"end":558,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_5.markups.4":{"type":"STRONG","start":560,"end":561,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_5.markups.5":{"type":"EM","start":318,"end":328,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_6":{"id":"2e6d156bec95_6","name":"7851","type":"P","href":null,"layout":null,"metadata":null,"text":"MDP consists of a tuple of 5 elements:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_6.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_6.markups.0":{"type":"STRONG","start":0,"end":3,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_7":{"id":"2e6d156bec95_7","name":"d8b8","type":"ULI","href":null,"layout":null,"metadata":null,"text":"S : Set of states. At each time step the state of the environment is an element s ‚àà S.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_7.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_7.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_7.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_7.markups.3","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_7.markups.0":{"type":"CODE","start":0,"end":1,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_7.markups.1":{"type":"CODE","start":80,"end":85,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_7.markups.2":{"type":"STRONG","start":0,"end":1,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_7.markups.3":{"type":"STRONG","start":84,"end":85,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_8":{"id":"2e6d156bec95_8","name":"273f","type":"ULI","href":null,"layout":null,"metadata":null,"text":"A: Set of actions. At each time step the agent choses an action a ‚àà A to perform.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_8.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_8.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_8.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_8.markups.3","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_8.markups.0":{"type":"CODE","start":0,"end":1,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_8.markups.1":{"type":"CODE","start":64,"end":69,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_8.markups.2":{"type":"STRONG","start":0,"end":1,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_8.markups.3":{"type":"STRONG","start":68,"end":69,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_9":{"id":"2e6d156bec95_9","name":"f62f","type":"ULI","href":null,"layout":null,"metadata":null,"text":"p(s_{t+1} | s_t, a_t) : State transition model that describes how the environment state changes when the user performs an action a depending on the action aand the current state s.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_9.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_9.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_9.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_9.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_9.markups.4","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_9.markups.0":{"type":"CODE","start":0,"end":21,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_9.markups.1":{"type":"CODE","start":129,"end":130,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_9.markups.2":{"type":"CODE","start":155,"end":156,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_9.markups.3":{"type":"STRONG","start":0,"end":21,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_9.markups.4":{"type":"STRONG","start":178,"end":179,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_10":{"id":"2e6d156bec95_10","name":"bc62","type":"ULI","href":null,"layout":null,"metadata":null,"text":"p(r_{t+1} | s_t, a_t) : Reward model that describes the real-valued reward value that the agent receives from the environment after performing an action. In MDP the the reward value depends on the current state and the action performed.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_10.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_10.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_10.markups.0":{"type":"CODE","start":0,"end":21,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_10.markups.1":{"type":"STRONG","start":0,"end":21,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_11":{"id":"2e6d156bec95_11","name":"e936","type":"ULI","href":null,"layout":null,"metadata":null,"text":"ùõæ : discount factor that controls the importance of future rewards. We will describe it in more details later.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_11.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_11.markups.0":{"type":"STRONG","start":0,"end":2,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_12":{"id":"2e6d156bec95_12","name":"4cd1","type":"P","href":null,"layout":null,"metadata":null,"text":"The way by which the agent chooses which action to perform is named the agent policy which is a function that takes the current environment state to return an action. The policy is often denoted by the symbol ùõë.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_12.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_12.markups.0":{"type":"CODE","start":78,"end":84,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_13":{"id":"2e6d156bec95_13","name":"320e","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*nS-MKI7pY8PeQLDCQ64xCA@2x.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*nS-MKI7pY8PeQLDCQ64xCA@2x.png":{"id":"1*nS-MKI7pY8PeQLDCQ64xCA@2x.png","originalHeight":74,"originalWidth":394,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:2e6d156bec95_14":{"id":"2e6d156bec95_14","name":"9956","type":"P","href":null,"layout":null,"metadata":null,"text":"Let‚Äôs now differentiate between two types environments.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_15":{"id":"2e6d156bec95_15","name":"7ca2","type":"P","href":null,"layout":null,"metadata":null,"text":"Deterministic environment: deterministic environment means that both state transition model and reward model are deterministic functions. If the agent while in a given state repeats a given action, it will always go the same next state and receive the same reward value.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_15.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_15.markups.0":{"type":"STRONG","start":0,"end":25,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_16":{"id":"2e6d156bec95_16","name":"7d25","type":"P","href":null,"layout":null,"metadata":null,"text":"Stochastic environment: In a stochastic environment there is uncertainty about the actions effect. When the agent repeats doing the same action in a given state, the new state and received reward may not be the same each time. For example, a robot which tries to move forward but because of the imperfection in the robot operation or other factors in the environment (e.g. slippery floor), sometimes the action forward will make it move forward but in sometimes, it will move to left or right.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_16.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_16.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_16.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_16.markups.3","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_16.markups.0":{"type":"CODE","start":411,"end":418,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_16.markups.1":{"type":"CODE","start":479,"end":483,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_16.markups.2":{"type":"CODE","start":487,"end":493,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_16.markups.3":{"type":"STRONG","start":0,"end":22,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_17":{"id":"2e6d156bec95_17","name":"59f8","type":"P","href":null,"layout":null,"metadata":null,"text":"Deterministic environments are easier to solve, because the agent knows how to plan its actions with no-uncertainty given the environment MDP. Possibly, the environment can be modeled in as a graph where each state is a node and edges represent transition actions from one state to another and edge weights are received rewards. Then, the agent can use a graph search algorithm such as A* to find the path with maximum total reward form the initial state.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_18":{"id":"2e6d156bec95_18","name":"b6d4","type":"H4","href":null,"layout":null,"metadata":null,"text":"Total reward","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_19":{"id":"2e6d156bec95_19","name":"eae3","type":"P","href":null,"layout":null,"metadata":null,"text":"Remember, that the goal of the agent is to pick the best policy that will maximize the total rewards received from the environment.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_20":{"id":"2e6d156bec95_20","name":"0ce8","type":"P","href":null,"layout":null,"metadata":null,"text":"Assume that environment is initially at state s_0","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_20.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_20.markups.0":{"type":"CODE","start":46,"end":49,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_21":{"id":"2e6d156bec95_21","name":"8366","type":"P","href":null,"layout":null,"metadata":null,"text":"At time 0 : Agent observes the environment state s_0 and picks an action a_0, then upon performing its action, environment state becomes s_1 and the agent receives a reward r_1 .","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_21.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_21.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_21.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_21.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_21.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_21.markups.5","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_21.markups.0":{"type":"CODE","start":8,"end":9,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_21.markups.1":{"type":"CODE","start":49,"end":52,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_21.markups.2":{"type":"CODE","start":73,"end":76,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_21.markups.3":{"type":"CODE","start":137,"end":140,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_21.markups.4":{"type":"CODE","start":173,"end":176,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_21.markups.5":{"type":"STRONG","start":0,"end":9,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_22":{"id":"2e6d156bec95_22","name":"235d","type":"P","href":null,"layout":null,"metadata":null,"text":"At time 1: Agent observes current state s_1 and picks an action a_1 , then upon acting its action, environment state becomes s_2 and it receives a reward r_2 .","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_22.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_22.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_22.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_22.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_22.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_22.markups.5","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_22.markups.0":{"type":"CODE","start":8,"end":9,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_22.markups.1":{"type":"CODE","start":40,"end":43,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_22.markups.2":{"type":"CODE","start":64,"end":67,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_22.markups.3":{"type":"CODE","start":125,"end":128,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_22.markups.4":{"type":"CODE","start":154,"end":157,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_22.markups.5":{"type":"STRONG","start":0,"end":9,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_23":{"id":"2e6d156bec95_23","name":"f3ac","type":"P","href":null,"layout":null,"metadata":null,"text":"At time 2: Agent observes current state s_2 and picks an action a_2 , then upon acting its action, environment state becomes s_3 and it receives a reward r_3 .","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_23.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_23.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_23.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_23.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_23.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_23.markups.5","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_23.markups.0":{"type":"CODE","start":8,"end":9,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_23.markups.1":{"type":"CODE","start":40,"end":43,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_23.markups.2":{"type":"CODE","start":64,"end":67,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_23.markups.3":{"type":"CODE","start":125,"end":128,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_23.markups.4":{"type":"CODE","start":154,"end":157,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_23.markups.5":{"type":"STRONG","start":0,"end":9,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_24":{"id":"2e6d156bec95_24","name":"e6ed","type":"P","href":null,"layout":null,"metadata":null,"text":"So the total reward received by the agent in response to the actions selected by its policy is going to be:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_25":{"id":"2e6d156bec95_25","name":"c2e2","type":"P","href":null,"layout":null,"metadata":null,"text":"Total reward = r_1 + r_2 + r_3 + r_4 + r_5 + ..","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_26":{"id":"2e6d156bec95_26","name":"d4ff","type":"P","href":null,"layout":null,"metadata":null,"text":"However, it is common to use a discount factor to give higher weight to near rewards received near than rewards received further in the future.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_27":{"id":"2e6d156bec95_27","name":"f4f8","type":"P","href":null,"layout":null,"metadata":null,"text":"Total discounted reward = r_1 + ùõæ r_2 + ùõæ¬≤ r_3 + ùõæ¬≥ r_4 + ùõæ‚Å¥ r_5+ ‚Ä¶\nso,","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_27.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_27.markups.0":{"type":"STRONG","start":31,"end":72,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_28":{"id":"2e6d156bec95_28","name":"14f5","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*Jix2ScBmffb1e5MpZCiwMg@2x.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*Jix2ScBmffb1e5MpZCiwMg@2x.png":{"id":"1*Jix2ScBmffb1e5MpZCiwMg@2x.png","originalHeight":424,"originalWidth":2336,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:2e6d156bec95_29":{"id":"2e6d156bec95_29","name":"caa6","type":"P","href":null,"layout":null,"metadata":null,"text":"where T is the horizon (episode length) which can be infinity if there is maximum length for the episode.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_29.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_29.markups.0":{"type":"CODE","start":6,"end":7,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_30":{"id":"2e6d156bec95_30","name":"6651","type":"P","href":null,"layout":null,"metadata":null,"text":"The reason for using discount factor is to prevent the total reward from going to infinity (because 0 ‚â§ ùõæ ‚â§ 1), it also models the agent behavior when the agent prefers immediate rewards than rewards that are potentially received far away in the future. (If I give you 1000 dollars today and If I give you 1000 days after 10 years which one would you prefer ? ). Imagine a robot that is trying to solve a maze and there are two paths to the goal state one of them is longer but gives higher reward while there is a shorter path with smaller reward. By adjusting the ùõæ value, you can control which the path the agent should prefer.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_30.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_30.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_30.markups.0":{"type":"CODE","start":100,"end":110,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_30.markups.1":{"type":"CODE","start":567,"end":569,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_31":{"id":"2e6d156bec95_31","name":"c3e5","type":"P","href":null,"layout":null,"metadata":null,"text":"Now, we are ready to introduce the value-iteration and policy-iteration algorithms. These are two fundamental methods for solving MDPs. Both value-iteration and policy-iteration assume that the agent knows the MDP model of the world (i.e. the agent knows the state-transition and reward probability functions). Therefore, they can be used by the agent to (offline) plan its actions given knowledge about the environment before interacting with it. Later, we will discuss Q-learning which is a model-free learning environment that can be used in situation where the agent initially knows only that are the possible states and actions but doesn't know the state-transition and reward probability functions. In Q-learning the agent improves its behavior (online) through learning from the history of interactions with the environment.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_31.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_31.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_31.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_31.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_31.markups.4","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_31.markups.0":{"type":"STRONG","start":35,"end":50,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_31.markups.1":{"type":"STRONG","start":55,"end":71,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_31.markups.2":{"type":"STRONG","start":471,"end":481,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_31.markups.3":{"type":"EM","start":356,"end":363,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_31.markups.4":{"type":"EM","start":752,"end":758,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_32":{"id":"2e6d156bec95_32","name":"c2d8","type":"H4","href":null,"layout":null,"metadata":null,"text":"Value function","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_33":{"id":"2e6d156bec95_33","name":"71bc","type":"P","href":null,"layout":null,"metadata":null,"text":"Many reinforcement learning introduce the notion of `value-function` which often denoted as V(s) . The value function represent how good is a state for an agent to be in. It is equal to expected total reward for an agent starting from state s. The value function depends on the policy by which the agent picks actions to perform. So, if the agent uses a given policy ùõë to select actions, the corresponding value function is given by:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_33.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_33.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_33.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_33.markups.0":{"type":"CODE","start":92,"end":96,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_33.markups.1":{"type":"CODE","start":241,"end":242,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_33.markups.2":{"type":"STRONG","start":53,"end":67,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_34":{"id":"2e6d156bec95_34","name":"a99f","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*V6MR4fgJnG1Sk-g8_k_Wug@2x.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*V6MR4fgJnG1Sk-g8_k_Wug@2x.png":{"id":"1*V6MR4fgJnG1Sk-g8_k_Wug@2x.png","originalHeight":424,"originalWidth":1896,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:2e6d156bec95_35":{"id":"2e6d156bec95_35","name":"9715","type":"P","href":null,"layout":null,"metadata":null,"text":"Among all possible value-functions, there exist an optimal value function that has higher value than other functions for all states.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_35.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_35.markups.0":{"type":"STRONG","start":51,"end":74,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_36":{"id":"2e6d156bec95_36","name":"368c","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*MMJt0UaDc_GrpY61Wb1dXw@2x.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*MMJt0UaDc_GrpY61Wb1dXw@2x.png":{"id":"1*MMJt0UaDc_GrpY61Wb1dXw@2x.png","originalHeight":200,"originalWidth":1752,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:2e6d156bec95_37":{"id":"2e6d156bec95_37","name":"1332","type":"P","href":null,"layout":null,"metadata":null,"text":"The optimal policy ùõë* is the policy that corresponds to optimal value function.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_38":{"id":"2e6d156bec95_38","name":"76b4","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*pO1aahVf3P8KLe5djev1jQ@2x.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*pO1aahVf3P8KLe5djev1jQ@2x.png":{"id":"1*pO1aahVf3P8KLe5djev1jQ@2x.png","originalHeight":200,"originalWidth":1796,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:2e6d156bec95_39":{"id":"2e6d156bec95_39","name":"1170","type":"P","href":null,"layout":null,"metadata":null,"text":"In addition to the state value-function, for convenience RL algorithms introduce another function which is the state-action pair Q function. Q is a function of a state-action pair and returns a real value.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_39.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_39.markups.0":{"type":"STRONG","start":129,"end":139,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_40":{"id":"2e6d156bec95_40","name":"8bb7","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*B4kn7-Tb7AJ4myyeBvpHcw@2x.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*B4kn7-Tb7AJ4myyeBvpHcw@2x.png":{"id":"1*B4kn7-Tb7AJ4myyeBvpHcw@2x.png","originalHeight":132,"originalWidth":912,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:2e6d156bec95_41":{"id":"2e6d156bec95_41","name":"e6a9","type":"P","href":null,"layout":null,"metadata":null,"text":"The optimal Q-function Q*(s, a) means the expected total reward received by an agent starting in sand picks action a, then will behave optimally afterwards. There, Q*(s, a) is an indication for how good it is for an agent to pick action a while being in state s.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_41.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_41.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_41.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_41.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_41.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_41.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_41.markups.6","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_41.markups.0":{"type":"CODE","start":23,"end":31,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_41.markups.1":{"type":"CODE","start":97,"end":98,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_41.markups.2":{"type":"CODE","start":115,"end":116,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_41.markups.3":{"type":"STRONG","start":23,"end":31,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_41.markups.4":{"type":"STRONG","start":97,"end":98,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_41.markups.5":{"type":"STRONG","start":115,"end":116,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_41.markups.6":{"type":"STRONG","start":164,"end":172,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_42":{"id":"2e6d156bec95_42","name":"b047","type":"P","href":null,"layout":null,"metadata":null,"text":"Since V*(s) is the maximum expected total reward when starting from state s , it will be the maximum of Q*(s, a)over all possible actions.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_42.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_42.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_42.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_42.markups.0":{"type":"CODE","start":6,"end":11,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_42.markups.1":{"type":"CODE","start":104,"end":112,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_42.markups.2":{"type":"STRONG","start":74,"end":75,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_43":{"id":"2e6d156bec95_43","name":"29dc","type":"P","href":null,"layout":null,"metadata":null,"text":"Therefore, the relationship between Q*(s, a) and V*(s) is easily obtained as:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_44":{"id":"2e6d156bec95_44","name":"b64f","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*mLQA-JCUiGRYs8H-O0VBKA@2x.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*mLQA-JCUiGRYs8H-O0VBKA@2x.png":{"id":"1*mLQA-JCUiGRYs8H-O0VBKA@2x.png","originalHeight":200,"originalWidth":1876,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:2e6d156bec95_45":{"id":"2e6d156bec95_45","name":"9af5","type":"P","href":null,"layout":null,"metadata":null,"text":"and If we know the optimal Q-function Q*(s, a) , the optimal policy can be easily extracted by choosing the action a that gives maximum Q*(s, a) for state s.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_45.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_45.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_45.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_45.markups.3","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_45.markups.0":{"type":"CODE","start":38,"end":46,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_45.markups.1":{"type":"CODE","start":115,"end":117,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_45.markups.2":{"type":"CODE","start":155,"end":156,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_45.markups.3":{"type":"STRONG","start":136,"end":145,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_46":{"id":"2e6d156bec95_46","name":"875d","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*YdSWi5JeQ83FWQeKk013-A@2x.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*YdSWi5JeQ83FWQeKk013-A@2x.png":{"id":"1*YdSWi5JeQ83FWQeKk013-A@2x.png","originalHeight":200,"originalWidth":2104,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:2e6d156bec95_47":{"id":"2e6d156bec95_47","name":"ed64","type":"P","href":null,"layout":null,"metadata":null,"text":"Now, lets introduce an important equation called the Bellman equation which is a super-important equation optimization and have applications in many fields such as reinforcement learning, economics and control theory. Bellman equation using dynamic programming paradigm provides a recursive definition for the optimal Q-function.\nThe Q*(s, a) is equal to the summation of immediate reward after performing action a while in state s and the discounted expected future reward after transition to a next state s'.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_47.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_47.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_47.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_47.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_47.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_47.markups.5","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_47.markups.0":{"type":"CODE","start":334,"end":342,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_47.markups.1":{"type":"CODE","start":413,"end":414,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_47.markups.2":{"type":"CODE","start":430,"end":431,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_47.markups.3":{"type":"CODE","start":507,"end":510,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_47.markups.4":{"type":"A","start":53,"end":69,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FBellman_equation","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_47.markups.5":{"type":"STRONG","start":52,"end":69,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_48":{"id":"2e6d156bec95_48","name":"3329","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*mAjQD3MsGTHGPBcBTf1Lyg@2x.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*mAjQD3MsGTHGPBcBTf1Lyg@2x.png":{"id":"1*mAjQD3MsGTHGPBcBTf1Lyg@2x.png","originalHeight":1348,"originalWidth":2920,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:2e6d156bec95_49":{"id":"2e6d156bec95_49","name":"5c61","type":"P","href":null,"layout":null,"metadata":null,"text":"Value-iteration and policy iteration rely on these equations to compute the optimal value-function.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_50":{"id":"2e6d156bec95_50","name":"297e","type":"H3","href":null,"layout":null,"metadata":null,"text":"Value Iteration","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_50.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_50.markups.0":{"type":"STRONG","start":0,"end":15,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_51":{"id":"2e6d156bec95_51","name":"9ae8","type":"P","href":null,"layout":null,"metadata":null,"text":"Value iteration computes the optimal state value function by iteratively improving the estimate of V(s). The algorithm initialize V(s) to arbitrary random values. It repeatedly updates the Q(s, a) and V(s) values until they converges. Value iteration is guaranteed to converge to the optimal values. This algorithm is shown in the following pseudo-code:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_51.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_51.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_51.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_51.markups.3","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_51.markups.0":{"type":"STRONG","start":99,"end":103,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_51.markups.1":{"type":"STRONG","start":130,"end":134,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_51.markups.2":{"type":"STRONG","start":189,"end":196,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_51.markups.3":{"type":"STRONG","start":201,"end":205,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_52":{"id":"2e6d156bec95_52","name":"10fd","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*MsD6og8hCReDO24T8iZfNw.png","typename":"ImageMetadata"},"text":"Pseudo code for value-iteration algorithm. Credit: Alpaydin Introduction to Machine Learning, 3rd edition.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_52.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*MsD6og8hCReDO24T8iZfNw.png":{"id":"1*MsD6og8hCReDO24T8iZfNw.png","originalHeight":334,"originalWidth":856,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:2e6d156bec95_52.markups.0":{"type":"STRONG","start":0,"end":106,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_53":{"id":"2e6d156bec95_53","name":"d6f4","type":"H4","href":null,"layout":null,"metadata":null,"text":"Example : FrozenLake8x8 (Using Value-Iteration)","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_54":{"id":"2e6d156bec95_54","name":"f378","type":"P","href":null,"layout":null,"metadata":null,"text":"Now lets implement it in python to solve the FrozenLake8x8 openAI gym. compared to the FrozenLake-v0 environment we solved earlier using genetic algorithm, the FrozenLake8x8 has 64 possible states (grid size is 8x8) instead of 16. Therefore, the problem becomes harder and genetic algorithm will struggle to find the optimal solution.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_54.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_54.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_54.markups.0":{"type":"A","start":45,"end":58,"href":"https:\u002F\u002Fgym.openai.com\u002Fenvs\u002FFrozenLake8x8-v0","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_54.markups.1":{"type":"A","start":160,"end":173,"href":"https:\u002F\u002Fgym.openai.com\u002Fenvs\u002FFrozenLake8x8-v0","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_55":{"id":"2e6d156bec95_55","name":"70b9","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"Solution of the FrozenLake8x8 environment using Value-Iteration","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":{"type":"id","generated":true,"id":"$Paragraph:2e6d156bec95_55.iframe","typename":"Iframe"},"mixtapeMetadata":null},"MediaResource:b0f0ac0bfbf315937689bc3c25944d53":{"id":"b0f0ac0bfbf315937689bc3c25944d53","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"Solution of FrozenLake8x8 environment using Value Iteration.","__typename":"MediaResource"},"$Paragraph:2e6d156bec95_55.iframe":{"mediaResource":{"type":"id","generated":false,"id":"MediaResource:b0f0ac0bfbf315937689bc3c25944d53","typename":"MediaResource"},"__typename":"Iframe"},"Paragraph:2e6d156bec95_56":{"id":"2e6d156bec95_56","name":"8033","type":"P","href":null,"layout":null,"metadata":null,"text":"Policy Iteration","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_56.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_56.markups.0":{"type":"STRONG","start":0,"end":16,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_57":{"id":"2e6d156bec95_57","name":"8992","type":"P","href":null,"layout":null,"metadata":null,"text":"While value-iteration algorithm keeps improving the value function at each iteration until the value-function converges. Since the agent only cares about the finding the optimal policy, sometimes the optimal policy will converge before the value function. Therefore, another algorithm called policy-iteration instead of repeated improving the value-function estimate, it will re-define the policy at each step and compute the value according to this new policy until the policy converges. Policy iteration is also guaranteed to converge to the optimal policy and it often takes less iterations to converge than the value-iteration algorithm.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_58":{"id":"2e6d156bec95_58","name":"a815","type":"P","href":null,"layout":null,"metadata":null,"text":"The pseudo code for Policy Iteration is shown below.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_59":{"id":"2e6d156bec95_59","name":"e0bd","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*WwOaLxFvDDgY0Uk92FO6Rw.png","typename":"ImageMetadata"},"text":"Pseudo code for policy-iteration algorithm. Credit: Alpaydin Introduction to Machine Learning, 3rd edition.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_59.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*WwOaLxFvDDgY0Uk92FO6Rw.png":{"id":"1*WwOaLxFvDDgY0Uk92FO6Rw.png","originalHeight":402,"originalWidth":974,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:2e6d156bec95_59.markups.0":{"type":"STRONG","start":0,"end":107,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_60":{"id":"2e6d156bec95_60","name":"cc19","type":"H4","href":null,"layout":null,"metadata":null,"text":"Example : FrozenLake8x8 (Using Policy-Iteration)","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_61":{"id":"2e6d156bec95_61","name":"ac9d","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"Solution of the FrozenLake8x8 environment using Policy Iteration","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":{"type":"id","generated":true,"id":"$Paragraph:2e6d156bec95_61.iframe","typename":"Iframe"},"mixtapeMetadata":null},"MediaResource:2ca8bcdc117cdb6e07013a590b3e78b4":{"id":"2ca8bcdc117cdb6e07013a590b3e78b4","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"frozenlake8x8_policyiteration.py","__typename":"MediaResource"},"$Paragraph:2e6d156bec95_61.iframe":{"mediaResource":{"type":"id","generated":false,"id":"MediaResource:2ca8bcdc117cdb6e07013a590b3e78b4","typename":"MediaResource"},"__typename":"Iframe"},"Paragraph:2e6d156bec95_62":{"id":"2e6d156bec95_62","name":"a980","type":"H4","href":null,"layout":null,"metadata":null,"text":"Value-Iteration vs Policy-Iteration","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_63":{"id":"2e6d156bec95_63","name":"4ab3","type":"P","href":null,"layout":null,"metadata":null,"text":"Both value-iteration and policy-iteration algorithms can be used for offline planning where the agent is assumed to have prior knowledge about the effects of its actions on the environment (they assume the MDP model is known). Comparing to each other, policy-iteration is computationally efficient as it often takes considerably fewer number of iterations to converge although each iteration is more computationally expensive.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_63.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_63.markups.0":{"type":"EM","start":69,"end":85,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_64":{"id":"2e6d156bec95_64","name":"41f9","type":"H3","href":null,"layout":null,"metadata":null,"text":"Q-Learning","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_65":{"id":"2e6d156bec95_65","name":"bc24","type":"P","href":null,"layout":null,"metadata":null,"text":"Now, lets consider the case where the agent does not know apriori what are the effects of its actions on the environment (state transition and reward models are not known). The agent only knows what are the set of possible states and actions, and can observe the environment current state. In this case, the agent has to actively learn through the experience of interactions with the environment. There are two categories of learning algorithms:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_66":{"id":"2e6d156bec95_66","name":"a8fe","type":"P","href":null,"layout":null,"metadata":null,"text":"model-based learning: In model-based learning, the agent will interact to the environment and from the history of its interactions, the agent will try to approximate the environment state transition and reward models. Afterwards, given the models it learnt, the agent can use value-iteration or policy-iteration to find an optimal policy.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_66.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_66.markups.0":{"type":"STRONG","start":0,"end":22,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_67":{"id":"2e6d156bec95_67","name":"70a0","type":"P","href":null,"layout":null,"metadata":null,"text":"model-free learning: in model-free learning, the agent will not try to learn explicit models of the environment state transition and reward functions. However, it directly derives an optimal policy from the interactions with the environment.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_67.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_67.markups.0":{"type":"STRONG","start":0,"end":20,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_68":{"id":"2e6d156bec95_68","name":"a24b","type":"P","href":null,"layout":null,"metadata":null,"text":"Q-Learning is an example of model-free learning algorithm. It does not assume that agent knows anything about the state-transition and reward models. However, the agent will discover what are the good and bad actions by trial and error.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_68.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_68.markups.0":{"type":"STRONG","start":0,"end":10,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_69":{"id":"2e6d156bec95_69","name":"4efa","type":"P","href":null,"layout":null,"metadata":null,"text":"The basic idea of Q-Learning is to approximate the state-action pairs Q-function from the samples of Q(s, a) that we observe during interaction with the enviornment. This approach is known as Time-Difference Learning.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_69.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_69.markups.0":{"type":"STRONG","start":192,"end":217,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_70":{"id":"2e6d156bec95_70","name":"9e94","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*yoEaeSssoS7tx-8H24yQQQ.png","typename":"ImageMetadata"},"text":"Q-Learning algorithm","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*yoEaeSssoS7tx-8H24yQQQ.png":{"id":"1*yoEaeSssoS7tx-8H24yQQQ.png","originalHeight":210,"originalWidth":1364,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:2e6d156bec95_71":{"id":"2e6d156bec95_71","name":"0354","type":"P","href":null,"layout":null,"metadata":null,"text":"where ùõÇ is the learning rate. The Q(s,a)table is initialized randomly. Then the agent starts to interact with the environment, and upon each interaction the agent will observe the reward of its action r(s,a)and the state transition (new state s'). The agent compute the observed Q-value Q_obs(s, a) and then use the above equation to update its own estimate of Q(s,a) .","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_71.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_71.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_71.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_71.markups.3","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_71.markups.0":{"type":"CODE","start":35,"end":41,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_71.markups.1":{"type":"CODE","start":202,"end":208,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_71.markups.2":{"type":"CODE","start":244,"end":246,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_71.markups.3":{"type":"CODE","start":362,"end":368,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_72":{"id":"2e6d156bec95_72","name":"b859","type":"P","href":null,"layout":null,"metadata":null,"text":"Exploration vs exploitation","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_72.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_72.markups.0":{"type":"STRONG","start":0,"end":27,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_73":{"id":"2e6d156bec95_73","name":"3029","type":"P","href":null,"layout":null,"metadata":null,"text":"An important question is how does the agent select actions during learning. Should the agent trust the learnt values of Q(s, a) enough to select actions based on it ? or try other actions hoping this may give it a better reward. This is known as the exploration vs exploitation dilemma.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_74":{"id":"2e6d156bec95_74","name":"432d","type":"P","href":null,"layout":null,"metadata":null,"text":"A simple approach is known as the ùõÜ-greedy approach where at each step. With small probability ùõú, the agent will pick a random action (explore) or with probability (1-ùõú) the agent will select an action according to the current estimate of Q-values. ùõú value can be decreased overtime as the agent becomes more confident with its estimate of Q-values.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_75":{"id":"2e6d156bec95_75","name":"edf0","type":"H4","href":null,"layout":null,"metadata":null,"text":"MountainCar Problem (using Q-Learning)","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_76":{"id":"2e6d156bec95_76","name":"a166","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*riQcuYHUPeg9adyv24kPHg.gif","typename":"ImageMetadata"},"text":"Solution of MountainCar Problem using Q-Learning","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*riQcuYHUPeg9adyv24kPHg.gif":{"id":"1*riQcuYHUPeg9adyv24kPHg.gif","originalHeight":400,"originalWidth":600,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:2e6d156bec95_77":{"id":"2e6d156bec95_77","name":"3bcc","type":"P","href":null,"layout":null,"metadata":null,"text":"Now, lets demonstrate how Q-Learning can be used to solve an interesting problem from OpenAI gym, the mountin-car problem. In the mountain car problem, there is a car on 1-dimensional track between two mountains. The goal of the car is to climb the mountain on its right. However, its engine is not strong to climb the mountain without having to go back to gain some momentum by climbing the mountain on the left.\nHere, the agent is the car, and possible actions are drive left, do nothing, or drive right. At every time step, the agent receives a penalty of -1 which means that the goal of the agent is to climb the right mountain as fast as possible to minimize the sum of -1 penalties it receives.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_77.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_77.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_77.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_77.markups.0":{"type":"CODE","start":559,"end":561,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_77.markups.1":{"type":"CODE","start":675,"end":677,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_77.markups.2":{"type":"A","start":102,"end":113,"href":"https:\u002F\u002Fgym.openai.com\u002Fenvs\u002FMountainCar-v0","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_78":{"id":"2e6d156bec95_78","name":"e465","type":"P","href":null,"layout":null,"metadata":null,"text":"The observation is two continuous variables representing the velocity and position of the car. Since, the observation variables are continuous, for our algorithm we discretize the observed values in order to use Q-Learning. \nWhile initially, the car is unable to climb the mountain and it will take forever, if you select a random action. After learning, it learns how to climb the mountain within less than 100 time-steps.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_78.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_78.markups.0":{"type":"CODE","start":408,"end":411,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_79":{"id":"2e6d156bec95_79","name":"f943","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"Solution of OpenAI Gym MountainCar Problem using Q-Learning.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":{"type":"id","generated":true,"id":"$Paragraph:2e6d156bec95_79.iframe","typename":"Iframe"},"mixtapeMetadata":null},"MediaResource:c0b047ca70a3c59b0d2b66ce83ca2c83":{"id":"c0b047ca70a3c59b0d2b66ce83ca2c83","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"Solution of MountainCar OpenAI Gym problem using Q-Learning.","__typename":"MediaResource"},"$Paragraph:2e6d156bec95_79.iframe":{"mediaResource":{"type":"id","generated":false,"id":"MediaResource:c0b047ca70a3c59b0d2b66ce83ca2c83","typename":"MediaResource"},"__typename":"Iframe"},"Paragraph:2e6d156bec95_80":{"id":"2e6d156bec95_80","name":"e22d","type":"H4","href":null,"layout":null,"metadata":null,"text":"References","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_81":{"id":"2e6d156bec95_81","name":"5656","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Alpaydin Introduction to Machine Learning, 3rd edition.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_82":{"id":"2e6d156bec95_82","name":"ba7c","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Stanford CS234 Reinforcement Learning","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_82.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_82.markups.0":{"type":"A","start":0,"end":37,"href":"http:\u002F\u002Fweb.stanford.edu\u002Fclass\u002Fcs234\u002Findex.html","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:2e6d156bec95_83":{"id":"2e6d156bec95_83","name":"fcfb","type":"ULI","href":null,"layout":null,"metadata":null,"text":"UC Berkley CS188 Introduction to AI","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:2e6d156bec95_83.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:2e6d156bec95_83.markups.0":{"type":"A","start":0,"end":35,"href":"http:\u002F\u002Fai.berkeley.edu\u002Fcourse_schedule.html","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Tag:machine-learning":{"id":"machine-learning","displayTitle":"Machine Learning","__typename":"Tag"},"Tag:reinforcement-learning":{"id":"reinforcement-learning","displayTitle":"Reinforcement Learning","__typename":"Tag"},"Tag:deep-learning":{"id":"deep-learning","displayTitle":"Deep Learning","__typename":"Tag"},"Tag:artificial-intelligence":{"id":"artificial-intelligence","displayTitle":"Artificial Intelligence","__typename":"Tag"},"Tag:openai":{"id":"openai","displayTitle":"OpenAI","__typename":"Tag"},"$Post:978f9e89ddaa.postResponses":{"count":0,"__typename":"PostResponses","responsesConnection({\"paging\":{\"limit\":10}})":{"type":"id","generated":true,"id":"$Post:978f9e89ddaa.postResponses.responsesConnection({\"paging\":{\"limit\":10}})","typename":"StreamConnection"}},"$Post:978f9e89ddaa.previewContent":{"subtitle":"In previous two articles, we introduced reinforcement learning definition, examples, and simple solving strategies using random policy‚Ä¶","__typename":"PreviewContent"},"$Post:978f9e89ddaa.postResponses.responsesConnection({\"paging\":{\"limit\":10}})":{"pagingInfo":null,"stream":[],"__typename":"StreamConnection"}}</script><script src="./good explanation of RL_files/manifest.dfcc068f.js.download"></script><script src="./good explanation of RL_files/vendors_main.240983a1.chunk.js.download"></script><script src="./good explanation of RL_files/main.85aba3db.chunk.js.download"></script><script src="./good explanation of RL_files/vendors_screen.collection.packageBuilder_screen.collection.styleEditor_screen.debug.cachedPost_scree_3171b25e.a9778a9a.chunk.js.download"></script>
<script src="./good explanation of RL_files/vendors_screen.collection.styleEditor_screen.debug.cachedPost_screen.post_screen.post.amp_screen.pos_f728b060.d9e25162.chunk.js.download"></script>
<script src="./good explanation of RL_files/screen.collection.packageBuilder_screen.collection.styleEditor_screen.debug.cachedPost_screen.landin_429769cf.7bbaee31.chunk.js.download"></script>
<script src="./good explanation of RL_files/screen.debug.cachedPost_screen.landingpages.tribute_screen.post_screen.post.amp_screen.profile_scree_92ba8b36.e30b3f6e.chunk.js.download"></script>
<script src="./good explanation of RL_files/screen.post.bb6d9334.chunk.js.download"></script><script>window.main();</script><script src="./good explanation of RL_files/p.js.download" async="" id="parsely-cf"></script></body></html>